# PTQ Benchmark Leaderboard

---

## Track A: Weight-Only Quantization (W4A16)

> **模型**: Qwen2.5-7B | **硬件**: H200 NVL 141GB

| 排名 | 方法 | PPL (WikiText-2) ↓ | Avg Accuracy ↑ | PPL 退化 | Acc 退化 | VRAM (MB) | 量化耗时 |
|:----:|------|:-------------------:|:--------------:|:--------:|:--------:|:---------:|:--------:|
| 🥇 | **FP16** (baseline) | **6.16** | **0.7351** | — | — | 18,367 | — |
| 🥈 | **AWQ** (W4, pre-quantized) | **6.91** | **0.7233** | +0.75 | -1.18% | 19,891 | 5.6s |
| 🥉 | **RTN** (W4, per-group sym) | **7.27** | **0.7098** | +1.11 | -2.53% | 21,822 | 0.1s |
| ⛔ | **GPTQ** (blocked) | — | — | — | — | — | — |

---

## Track C: KV Cache Compression

> **模型**: Qwen2.5-7B (7.62B params) | **硬件**: H200 NVL 141GB | **日期**: 2026-02-17

### 什么是 KV Cache 压缩？

大语言模型 (LLM) 在生成文本时，需要缓存之前所有 token 的 Key 和 Value 向量 (简称 **KV Cache**)，以便后续 token 通过 Attention 机制"回看"历史信息。

**问题**: 随着对话/文本变长，KV Cache 占用的显存线性增长。例如 7B 模型在 128K 上下文时，KV Cache 占用 ~8GB。

**解决方案**: KV Cache 压缩方法在不(显著)影响模型质量的前提下，减小 KV Cache 的存储开销。

### 对比的 3 种方法

| 方法 | 压缩原理 | 关键参数 | 免校准 |
|------|---------|---------|:------:|
| **KIVI** | 将 KV 向量量化为 **INT2** (2-bit 整数)。Key 按 channel 量化，Value 按 token 量化。最近 `residual_length` 个 token 保持 FP16 不压缩 | `bits=2`, `group_size=32`, `residual_length` | ✅ |
| **KVQuant** | 同样 INT2 量化，但额外**隔离异常值** (outlier): 每个向量中绝对值最大的 `num_outliers` 个元素单独以 FP16 存储，剩余再量化 | `bits=2`, `outliers=1`, `residual_length` | ✅ |
| **FORGE** | 将 KV 按固定 chunk 分块，每块做 **SVD 分解** (奇异值分解)，只保留能量占比最高的前几个主成分。动态决定每块保留的秩 (rank) | `chunk=64`, `energy=0.999`, `max_rank=64` | ✅ |

> **`residual_length`** (KIVI/KVQuant 特有): 最近的 N 个 token 保持原始 FP16 精度不压缩，作为"保护区"。值越大 → 保护区越大 → 质量越好但压缩比越低。
>
> **`energy_threshold`** (FORGE 特有): SVD 保留多少"能量"(信息量)。0.999 表示保留 99.9% 的能量。值越高 → 保留的秩越多 → 质量越好但压缩比越低。

---

### 评测指标说明

我们用两个指标评测 KV Cache 压缩的质量:

#### 📌 Passkey Retrieval (精确记忆能力)

> **"模型能否从长文本中精确回忆出一个嵌入在垃圾文本里的 5 位密钥？"**

**做法**: 在大量无关废话文本中随机插入一句 "The passkey is 54321"，然后在末尾问 "What is the passkey?"，看模型能否原样输出 "54321"。

**为什么用它**: 这个任务要求模型精确检索远处的具体信息，任何 KV Cache 中的信息丢失都会直接导致检索失败。

**设置**:
- 每种条件测试 **10 个随机密钥 × 4 个插入深度** (10%, 25%, 50%, 75%) = **40 次测试**
- 深度 = 密钥在文本中的相对位置 (10% = 靠前, 75% = 靠后)

**解读**: **100%** = 完美，与 FP16 一致；**0%** = 彻底失败。

#### 📌 Generation PPL (生成流畅度)

> **"模型生成的文本在自己看来有多'自然'？"**

**做法**: 给模型一段 prompt，让它 greedy decode 生成 1K token，然后对生成的文本计算 perplexity (困惑度)。

**为什么用它**: 如果 KV Cache 压缩导致模型"忘记"了自己之前说的内容，生成的连贯性会下降，PPL 会升高。

**注意**: Greedy decoding 生成的文本天然在 teacher-forcing 下具有高自洽性 (PPL ≈ 1.0-1.2)，所以这个指标灵敏度较低，只能检测**严重**退化。

**解读**: **越低越好**。FP16 baseline ≈ 1.20。

---

### Passkey Retrieval 结果

> 10 个 5 位密钥 × 4 个深度 = 每格 40 次测试

| 条件 | Context | Residual | **FP16** | **KIVI** | **KVQuant** | **FORGE** |
|------|:-------:|:--------:|:--------:|:--------:|:-----------:|:---------:|
| 温和 | 2K | 128 | **100%** | 45% | 72% | **100%** |
| 中等 | 2K | 32 | **100%** | 32% | 68% | **100%** |
| 温和+长 | 4K | 128 | **100%** | 22% | 38% | **100%** |
| 中等+长 | 4K | 32 | **100%** | 38% | 35% | **100%** |
| 长上下文 | 8K | 64 | **100%** | 10% | 28% | **100%** |
| 激进 | 8K | 32 | **100%** | 5% | 42% | **100%** |

#### 趋势解读

```
FP16:    ████████████████████ 100%  (所有条件)
FORGE:   ████████████████████ 100%  (所有条件, energy=0.999)
KVQuant: ██████████████       72% → 28%  (随 context 增大下降)
KIVI:    █████████            45% →  5%  (下降最快)
```

### Generation PPL 结果

> prompt=2K, gen=1K, residual_length=32

| 方法 | 样本 0 | 样本 1 | 样本 2 | **平均** |
|------|:------:|:------:|:------:|:--------:|
| **FP16** | 1.07 | 1.28 | 1.23 | **1.20** |
| **KIVI** | 55258 ⚠️ | 1.14 | 1.30 | **18420** |
| **KVQuant** | 1.24 | 1.24 | 1.11 | **1.20** |
| **FORGE** | 1.10 | 1.35 | 1.11 | **1.19** |

> ⚠️ KIVI 样本 0 的 PPL=55258 是因为模型在生成第 1 个 token 后就输出了 EOS (结束符)，只生成了 1 个 token，PPL 等于该 token 的 cross-entropy loss。这说明 INT2 量化导致的 KV 误差有时会严重干扰生成。

---

### 方法排名

| 排名 | 方法 | Passkey (最佳/最差) | Gen PPL | 速度开销 | 总评 |
|:----:|------|:---:|:---:|:---:|------|
| 🥇 | **FORGE** (SVD, e=0.999) | 100% / 100% | 1.19 | 🐢 ~33min/8K | 质量完美，但 SVD 计算极慢 |
| 🥈 | **KVQuant** (INT2+outlier) | 72% / 28% | 1.20 | ⚡ ~19s/8K | outlier 隔离有效，中等退化 |
| 🥉 | **KIVI** (INT2) | 45% / 5% | 18420 | ⚡ ~17s/8K | 严重退化，偶发生成崩溃 |

---

### 深层分析

#### 为什么 FORGE 反而最好？

FORGE 用 SVD 保留 99.9% 的能量，avg_rank ≈ 44 (head_dim=128)，相当于保留了 34% 的维度。虽然丢了 0.1% 的"能量"，但这些丢掉的成分主要是噪声和高频细节，对 Attention 的影响极小。

**关键**: energy=0.999 时，重构误差 (相对) 仅 ~1.8%，远低于 KIVI INT2 的 ~3.2%。

#### 为什么 KIVI 最差？

INT2 (2-bit) 只有 4 个量化级别 (0, 1, 2, 3)。对于分布范围大的 KV 向量，4 个级别远远不够精确表示。且没有 outlier 保护。

#### 为什么 KVQuant 比 KIVI 好？

每个向量隔离 1 个最大 outlier 并保持 FP16，只 quantize 剩余值。这避免了 outlier 拉大量化范围导致的精度损失 — 即所谓 "dense-and-sparse" 策略。

#### FORGE 的代价

SVD 分解的计算开销极大: 在 8K context 上每个 Passkey 测试需要 ~33 分钟 (vs KIVI/KVQuant ~19 秒)。这是 **~100 倍的速度差距**。实际部署中需要高度优化的 SVD kernel 或近似算法。

---

## 环境信息

| 项目 | 版本 |
|------|------|
| GPU | NVIDIA H200 NVL 141GB |
| PyTorch | 2.10.0+cu128 |
| Transformers | 5.1.0 |
| lm-eval | 0.4.11 |
| Python | 3.13 |
