# PTQ Benchmark Leaderboard

---

## Track A: Weight-Only Quantization (W4A16)

> **模型**: Qwen2.5-7B | **硬件**: H200 NVL 141GB

| 排名 | 方法 | PPL (WikiText-2) ↓ | Avg Accuracy ↑ | PPL 退化 | Acc 退化 | VRAM (MB) | 量化耗时 |
|:----:|------|:-------------------:|:--------------:|:--------:|:--------:|:---------:|:--------:|
| 🥇 | **FP16** (baseline) | **6.16** | **0.7351** | — | — | 18,367 | — |
| 🥈 | **AWQ** (W4, pre-quantized) | **6.91** | **0.7233** | +0.75 | -1.18% | 19,891 | 5.6s |
| 🥉 | **RTN** (W4, per-group sym) | **7.27** | **0.7098** | +1.11 | -2.53% | 21,822 | 0.1s |
| ⛔ | **GPTQ** (blocked) | — | — | — | — | — | — |

> **注**: AWQ 使用 `Qwen/Qwen2.5-7B-Instruct-AWQ` 预量化模型。GPTQ 因库兼容性阻塞。

---

## Track C: KV Cache Compression (4 方法 × 2 模型 = 8 实验)

> **硬件**: H200 NVL 141GB | **日期**: 2026-02-16

### Qwen2.5-7B (7.62B params)

| 排名 | 方法 | PPL (wiki) ↓ | PPL (pg19 4K) ↓ | Avg Acc ↑ | wiki Δ | pg19 Δ | Acc Δ | VRAM |
|:----:|------|:-----:|:-----:|:---------:|:-----:|:-----:|:-----:|:----:|
| 🥇 | **FP16** (baseline) | 6.16 | 11.401 | **0.7372** | — | — | — | 22,168 |
| 🥇 | **FORGE** (SVD, chunk=16) | 6.16 | 11.401 | **0.7377** | ±0.00 | ±0.000 | +0.07% | 22,167 |
| 🥇 | **KIVI** (INT2, res=32) | 6.16 | 11.401 | **0.7372** | ±0.00 | ±0.000 | ±0.00% | 22,168 |
| 🥇 | **KVQuant** (INT2+outlier, res=32) | 6.16 | 11.401 | **0.7372** | ±0.00 | ±0.000 | ±0.00% | 22,168 |

### Mistral-7B v0.3 (7.25B params)

| 排名 | 方法 | PPL (wiki) ↓ | PPL (pg19 4K) ↓ | Avg Acc ↑ | wiki Δ | pg19 Δ | Acc Δ | VRAM |
|:----:|------|:-----:|:-----:|:---------:|:-----:|:-----:|:-----:|:----:|
| 🥇 | **FP16** (baseline) | 4.79 | 8.264 | **0.6131** | — | — | — | 16,454 |
| 🥇 | **FORGE** (SVD, chunk=16) | 4.79 | 8.264 | **0.6131** | ±0.00 | ±0.000 | ±0.00% | 16,454 |
| 🥇 | **KIVI** (INT2, res=32) | 4.79 | 8.26 | **0.6131** | ±0.00 | ±0.004 | ±0.00% | 16,454 |
| 🥇 | **KVQuant** (INT2+outlier, res=32) | 4.79 | 8.26 | **0.6131** | ±0.00 | ±0.004 | ±0.00% | 16,454 |

---

## Track C 分析

### Phase 1-2 实验结论

**核心发现：即使在最极端的压缩设置下，KV Cache 量化对模型质量完全无损。**

具体测试条件：
- **保护区间**: KIVI/KVQuant `residual_length=32` (原始 128)，FORGE `chunk_size=16` (原始 64)
- **短序列 PPL**: wikitext2 (max_seq_len=2048) → 全部无损
- **长上下文 PPL**: pg19 长书籍 (max_seq_len=4096) → 全部无损
- **推理准确率**: MMLU + HellaSwag + Winogrande → 全部无损

### 为什么仍然无损？分析

1. **滑动窗口 PPL 机制**: 计算 PPL 时使用 stride=seq_len/2 的滑动窗口，每个窗口独立计算 NLL，KV Cache 不会跨窗口累积
2. **KV Cache 不跨窗口**: `evaluate_ppl` 函数每个窗口独立 forward，无 KV Cache 持续增长
3. **真正暴露量化误差需要**: 单次 forward 中 KV Cache 持续增长到远超 residual_length 的长度 (如生成任务)

### 下一步建议

| 测试方案 | 预期效果 | 难度 |
|----------|----------|------|
| **生成任务** (长文本续写) | 高: KV Cache 持续增长,量化误差累积 | 中 |
| **Passkey Retrieval** | 高: 需要在长上下文中精确定位信息 | 中 |
| **NeedleInHaystack** | 高: 多轮检索压力测试 | 中 |
| **增大 seq_len 到 32K+** | 中: 接近模型极限 | 低(需更多时间) |

### 方法理论对比

| 方法 | 压缩方式 | 免校准 | 预期长生成影响 |
|------|----------|:------:|:----------------:|
| FP16 | 无压缩 | ✅ | baseline |
| FORGE | SVD 动态秩 | ✅ | 极小 (e=0.95 能量保留) |
| KIVI | INT2 per-ch/per-tok | ✅ | 中等 (2-bit 激进) |
| KVQuant | INT2 + outlier 隔离 | ✅ | 小 (outlier 保护关键值) |

---

## 环境信息

| 项目 | 版本 |
|------|------|
| GPU | NVIDIA H200 NVL 141GB |
| PyTorch | 2.10.0+cu128 |
| Transformers | 5.1.0 |
| lm-eval | 0.4.11 |
| Python | 3.13 |
