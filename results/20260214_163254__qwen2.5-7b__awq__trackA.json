{
  "meta": {
    "filename": "20260214_163254__qwen2.5-7b__awq__trackA",
    "timestamp": "2026-02-14T16:32:54.311774",
    "script": "scripts/run_one.py",
    "cli_args": "scripts/run_one.py --model qwen2.5-7b --method awq --track A",
    "model": "qwen2.5-7b",
    "method": "awq",
    "track": "A"
  },
  "config": {
    "env": {
      "conda_env": "ptq-bench",
      "python": "3.11",
      "cuda": "12.8",
      "torch": "2.10.0+cu128",
      "transformers": "4.52",
      "lm_eval": "0.4.11",
      "vllm": null
    },
    "paths": {
      "project_root": "/data2/zhu11/ptq-bench",
      "data_root": "/data2/zhu11/quant_source/data",
      "results_root": "/data2/zhu11/ptq-bench/results",
      "plots_root": "/data2/zhu11/ptq-bench/plots",
      "cache_dir": "/data2/zhu11/quant_source/models",
      "model_cache_dir": "/data2/zhu11/quant_source/models"
    },
    "common_hyperparams": {
      "seed": 42,
      "max_seq_len": 2048,
      "dtype": "float16",
      "batch_size": 1,
      "eval_default_fewshot": {
        "mmlu": 5,
        "gsm8k": 8,
        "hellaswag": 10,
        "winogrande": 5,
        "arc_easy": 25,
        "arc_challenge": 25,
        "piqa": 0
      }
    },
    "default_calibration": {
      "dataset": "wikitext2",
      "num_samples": 128,
      "seq_len": 2048,
      "packing": true,
      "seed": 42
    },
    "default_eval": {
      "core_quality": {
        "ppl_datasets": [
          "wikitext2"
        ],
        "lm_eval_tasks": [
          "mmlu",
          "hellaswag",
          "winogrande",
          "arc_easy",
          "arc_challenge",
          "piqa",
          "gsm8k"
        ]
      },
      "system_metrics": {
        "enabled": false,
        "batch_sizes": [
          1,
          8,
          32
        ],
        "prompt_lens": [
          128,
          512,
          2048
        ],
        "gen_lens": [
          128,
          256
        ]
      },
      "long_context": {
        "enabled": false,
        "max_seq_len": 32768,
        "tasks": [
          "longbench"
        ]
      }
    },
    "name": "awq",
    "display_name": "AWQ",
    "description": "权重 4-bit 量化，激活不量化。覆盖 RTN/GPTQ/AWQ/OmniQuant/SpQR 等方法。",
    "constraints": {
      "weight_bits": 4,
      "activation_bits": 16,
      "kv_quantize": false
    },
    "eval": {
      "ppl_datasets": [
        "wikitext2"
      ],
      "lm_eval_tasks": [
        "mmlu",
        "hellaswag",
        "winogrande",
        "arc_easy",
        "arc_challenge",
        "piqa",
        "gsm8k"
      ],
      "primary_metric": "avg_accuracy",
      "system_metrics": false
    },
    "supported_tracks": [
      "A"
    ],
    "library": "autoawq",
    "wrapper": "awq",
    "weight": {
      "w_bits": 4,
      "group_size": 128,
      "granularity": "per_group",
      "scheme": "asymmetric",
      "zero_point": true,
      "version": "gemm"
    },
    "calibration": {
      "required": true,
      "dataset": "wikitext2",
      "num_samples": 128,
      "seq_len": 2048,
      "packing": true,
      "seed": 42
    },
    "model": {
      "name": "qwen2.5-7b",
      "model_id": "Qwen/Qwen2.5-7B",
      "dtype": "bfloat16",
      "max_seq_len": 131072,
      "trust_remote_code": false,
      "revision": null,
      "tokenizer_id": null,
      "adapter": null,
      "attn_implementation": null,
      "model_kwargs": {},
      "pretrained_quant_models": {
        "awq": "Qwen/Qwen2.5-7B-Instruct-AWQ",
        "gptq": "Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4"
      }
    },
    "track": "A"
  },
  "results": {
    "ppl": {
      "wikitext2": {
        "ppl": 6.9134,
        "nll": 1.933458,
        "num_windows": 291
      }
    },
    "lm_eval": {
      "arc_challenge": {
        "alias": "arc_challenge",
        "acc,none": 0.5162,
        "acc_stderr,none": 0.0146,
        "acc_norm,none": 0.5418,
        "acc_norm_stderr,none": 0.0146
      },
      "arc_easy": {
        "alias": "arc_easy",
        "acc,none": 0.8106,
        "acc_stderr,none": 0.008,
        "acc_norm,none": 0.7854,
        "acc_norm_stderr,none": 0.0084
      },
      "gsm8k": {
        "alias": "gsm8k",
        "exact_match,strict-match": 0.7642,
        "exact_match_stderr,strict-match": 0.0117,
        "exact_match,flexible-extract": 0.8089,
        "exact_match_stderr,flexible-extract": 0.0108
      },
      "hellaswag": {
        "alias": "hellaswag",
        "acc,none": 0.6145,
        "acc_stderr,none": 0.0049,
        "acc_norm,none": 0.7954,
        "acc_norm_stderr,none": 0.004
      },
      "mmlu": {
        "acc,none": 0.708,
        "acc_stderr,none": 0.0036,
        "alias": "mmlu"
      },
      "mmlu_humanities": {
        "acc,none": 0.6257,
        "acc_stderr,none": 0.0066,
        "alias": " - humanities"
      },
      "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.5476,
        "acc_stderr,none": 0.0445
      },
      "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.8364,
        "acc_stderr,none": 0.0289
      },
      "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8775,
        "acc_stderr,none": 0.023
      },
      "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.8608,
        "acc_stderr,none": 0.0225
      },
      "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8264,
        "acc_stderr,none": 0.0346
      },
      "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.8148,
        "acc_stderr,none": 0.0376
      },
      "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.8037,
        "acc_stderr,none": 0.0312
      },
      "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.763,
        "acc_stderr,none": 0.0229
      },
      "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.4212,
        "acc_stderr,none": 0.0165
      },
      "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.7299,
        "acc_stderr,none": 0.0252
      },
      "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.8086,
        "acc_stderr,none": 0.0219
      },
      "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4961,
        "acc_stderr,none": 0.0128
      },
      "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8421,
        "acc_stderr,none": 0.028
      },
      "mmlu_other": {
        "acc,none": 0.7599,
        "acc_stderr,none": 0.0073,
        "alias": " - other"
      },
      "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.82,
        "acc_stderr,none": 0.0386
      },
      "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7811,
        "acc_stderr,none": 0.0254
      },
      "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.7052,
        "acc_stderr,none": 0.0348
      },
      "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.45,
        "acc_stderr,none": 0.05
      },
      "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.7534,
        "acc_stderr,none": 0.0289
      },
      "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8738,
        "acc_stderr,none": 0.0329
      },
      "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.906,
        "acc_stderr,none": 0.0191
      },
      "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.82,
        "acc_stderr,none": 0.0386
      },
      "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8467,
        "acc_stderr,none": 0.0129
      },
      "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.8007,
        "acc_stderr,none": 0.0229
      },
      "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.5355,
        "acc_stderr,none": 0.0298
      },
      "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.7647,
        "acc_stderr,none": 0.0258
      },
      "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5181,
        "acc_stderr,none": 0.0389
      },
      "mmlu_social_sciences": {
        "acc,none": 0.8148,
        "acc_stderr,none": 0.0069,
        "alias": " - social sciences"
      },
      "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.6491,
        "acc_stderr,none": 0.0449
      },
      "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.8939,
        "acc_stderr,none": 0.0219
      },
      "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.9171,
        "acc_stderr,none": 0.0199
      },
      "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.7718,
        "acc_stderr,none": 0.0213
      },
      "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.8445,
        "acc_stderr,none": 0.0235
      },
      "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8972,
        "acc_stderr,none": 0.013
      },
      "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7863,
        "acc_stderr,none": 0.036
      },
      "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.7533,
        "acc_stderr,none": 0.0174
      },
      "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7273,
        "acc_stderr,none": 0.0427
      },
      "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7388,
        "acc_stderr,none": 0.0281
      },
      "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8806,
        "acc_stderr,none": 0.0229
      },
      "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.86,
        "acc_stderr,none": 0.0349
      },
      "mmlu_stem": {
        "acc,none": 0.6755,
        "acc_stderr,none": 0.0081,
        "alias": " - stem"
      },
      "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.51,
        "acc_stderr,none": 0.0502
      },
      "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.7407,
        "acc_stderr,none": 0.0379
      },
      "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.8553,
        "acc_stderr,none": 0.0286
      },
      "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.8472,
        "acc_stderr,none": 0.0301
      },
      "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.55,
        "acc_stderr,none": 0.05
      },
      "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.6,
        "acc_stderr,none": 0.0492
      },
      "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.48,
        "acc_stderr,none": 0.0502
      },
      "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.5098,
        "acc_stderr,none": 0.0497
      },
      "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.78,
        "acc_stderr,none": 0.0416
      },
      "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.7489,
        "acc_stderr,none": 0.0283
      },
      "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6621,
        "acc_stderr,none": 0.0394
      },
      "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.6587,
        "acc_stderr,none": 0.0244
      },
      "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.8581,
        "acc_stderr,none": 0.0199
      },
      "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.6502,
        "acc_stderr,none": 0.0336
      },
      "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.8,
        "acc_stderr,none": 0.0402
      },
      "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.5333,
        "acc_stderr,none": 0.0304
      },
      "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.543,
        "acc_stderr,none": 0.0407
      },
      "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.7037,
        "acc_stderr,none": 0.0311
      },
      "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.5089,
        "acc_stderr,none": 0.0475
      },
      "piqa": {
        "alias": "piqa",
        "acc,none": 0.7742,
        "acc_stderr,none": 0.0098,
        "acc_norm,none": 0.7856,
        "acc_norm_stderr,none": 0.0096
      },
      "winogrande": {
        "alias": "winogrande",
        "acc,none": 0.6977,
        "acc_stderr,none": 0.0129
      },
      "_avg_accuracy": 0.7233
    },
    "system_metrics": {
      "vram_peak_mb": 19890.8
    },
    "eval_time_seconds": 5798.8,
    "quant_time_seconds": 5.6
  },
  "env": {
    "timestamp": "2026-02-14T16:32:54.284917",
    "timestamp_utc": "2026-02-14T21:32:54.284924+00:00",
    "hostname": "sev-cxl",
    "os": "Linux 6.11.0-snp-host-68799c0277b2",
    "python_version": "3.13.11 | packaged by Anaconda, Inc. | (main, Dec 10 2025, 21:28:48) [GCC 14.3.0]",
    "git": {
      "commit_hash": null,
      "branch": null,
      "is_dirty": null
    },
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA H200 NVL",
        "memory_total_mb": 143771,
        "driver_version": "580.95.05",
        "compute_capability": "9.0"
      }
    ],
    "cuda": {
      "cuda_available": true,
      "cuda_version": "12.8",
      "cudnn_version": "91002",
      "torch_cuda_arch_list": null
    },
    "packages": {
      "torch": "2.10.0+cu128",
      "transformers": "5.1.0",
      "datasets": "4.5.0",
      "accelerate": "1.12.0",
      "auto-gptq": "not installed",
      "autoawq": "not installed",
      "smoothquant": "not installed",
      "lm-eval": "0.4.11",
      "vllm": "not installed",
      "safetensors": "0.7.0",
      "tokenizers": "0.22.2",
      "bitsandbytes": "not installed",
      "scipy": "1.17.0",
      "numpy": "2.4.2"
    }
  },
  "warnings": []
}