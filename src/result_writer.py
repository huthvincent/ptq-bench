# -*- coding: utf-8 -*-
"""
ç»“æœå†™å…¥å™¨

è´Ÿè´£å°†å®éªŒç»“æœå†™å…¥ .mdï¼ˆäººç±»é˜…è¯»ï¼‰ å’Œ .jsonï¼ˆæœºå™¨è§£æï¼‰ ä¸¤ç§æ ¼å¼ã€‚

å‘½åè§„åˆ™ï¼š
    YYYYMMDD_HHMMSS__{model}__{method}__{track}.md
    YYYYMMDD_HHMMSS__{model}__{method}__{track}.json
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

from src.env_info import collect_env_info, format_env_info


def generate_result_filename(model_name: str, method_name: str, track: str) -> str:
    """
    ç”Ÿæˆç»“æœæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ã€‚

    æ ¼å¼: YYYYMMDD_HHMMSS__{model}__{method}__{track}

    å‚æ•°:
        model_name: æ¨¡å‹åç§°
        method_name: æ–¹æ³•åç§°
        track: èµ›é“åç§°

    è¿”å›:
        str: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åå’Œç›®å½•è·¯å¾„ï¼‰
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{timestamp}__{model_name}__{method_name}__track{track.upper()}"


def write_results(
    results: dict,
    config: dict,
    output_dir: str | Path,
    cli_args: str = "",
    script_name: str = "",
    warnings: list[str] | None = None,
) -> tuple[Path, Path]:
    """
    å°†å®éªŒç»“æœå†™å…¥ .md å’Œ .json æ–‡ä»¶ã€‚

    å‚æ•°:
        results: è¯„æµ‹ç»“æœå­—å…¸ï¼ˆæ¥è‡ª evaluator.evaluate()ï¼‰
        config: åˆå¹¶åçš„å®Œæ•´é…ç½®å­—å…¸
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        cli_args: å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°å­—ç¬¦ä¸²
        script_name: è¿è¡Œçš„è„šæœ¬åç§°
        warnings: è¿è¡Œè¿‡ç¨‹ä¸­çš„è­¦å‘Šä¿¡æ¯åˆ—è¡¨

    è¿”å›:
        tuple[Path, Path]: (md_path, json_path) ä¸¤ä¸ªæ–‡ä»¶çš„è·¯å¾„
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    model_name = config.get("model", {}).get("name", "unknown")
    method_name = config.get("name", "unknown")
    track = config.get("track", "?")

    basename = generate_result_filename(model_name, method_name, track)
    md_path = output_dir / f"{basename}.md"
    json_path = output_dir / f"{basename}.json"

    # æ”¶é›†ç¯å¢ƒä¿¡æ¯
    env_info = collect_env_info()

    # --- å†™å…¥ JSON ---
    json_data = {
        "meta": {
            "filename": basename,
            "timestamp": datetime.now().isoformat(),
            "script": script_name,
            "cli_args": cli_args,
            "model": model_name,
            "method": method_name,
            "track": track,
        },
        "config": config,
        "results": results,
        "env": env_info,
        "warnings": warnings or [],
    }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)

    # --- å†™å…¥ Markdown ---
    md_content = _render_markdown(json_data)
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_content)

    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜:")
    print(f"   MD:   {md_path}")
    print(f"   JSON: {json_path}")

    return md_path, json_path


def _render_markdown(data: dict) -> str:
    """
    å°†ç»“æœæ•°æ®æ¸²æŸ“ä¸º Markdown æ ¼å¼ã€‚

    å‚æ•°:
        data: åŒ…å« metaã€configã€resultsã€env çš„å®Œæ•´æ•°æ®å­—å…¸

    è¿”å›:
        str: æ ¼å¼åŒ–çš„ Markdown å†…å®¹
    """
    meta = data["meta"]
    config = data["config"]
    results = data["results"]
    env_info = data["env"]
    warnings = data.get("warnings", [])

    lines = []

    # === æ ‡é¢˜ ===
    lines.append(f"# å®éªŒç»“æœ: {meta['model']} + {meta['method']} (Track {meta['track']})")
    lines.append("")

    # === è­¦å‘Š ===
    if warnings:
        for w in warnings:
            lines.append(f"> âš ï¸ **è­¦å‘Š**: {w}")
        lines.append("")

    # === è¿è¡Œä¿¡æ¯ ===
    lines.append("## è¿è¡Œä¿¡æ¯")
    lines.append("")
    lines.append(f"- **è¿è¡Œæ—¶é—´**: {meta.get('timestamp', 'N/A')}")
    lines.append(f"- **è„šæœ¬**: `{meta.get('script', 'N/A')}`")
    lines.append(f"- **å®Œæ•´ CLI å‚æ•°**:")
    lines.append(f"  ```")
    lines.append(f"  {meta.get('cli_args', 'N/A')}")
    lines.append(f"  ```")
    lines.append("")

    # === æ•°æ®é›†ä¿¡æ¯ ===
    lines.append("## æ•°æ®é›†")
    lines.append("")
    calib = config.get("calibration", config.get("default_calibration", {}))
    lines.append(f"- **æ ¡å‡†æ•°æ®é›†**: {calib.get('dataset', 'N/A')}")
    lines.append(f"- **æ ¡å‡†æ ·æœ¬æ•°**: {calib.get('num_samples', 'N/A')}")
    lines.append(f"- **æ ¡å‡†åºåˆ—é•¿åº¦**: {calib.get('seq_len', 'N/A')}")

    eval_config = config.get("eval", config.get("default_eval", {}))
    core = eval_config.get("core_quality", {})
    lines.append(f"- **PPL è¯„æµ‹æ•°æ®é›†**: {', '.join(core.get('ppl_datasets', []))}")
    lines.append(f"- **lm-eval ä»»åŠ¡**: {', '.join(core.get('lm_eval_tasks', []))}")
    lines.append("")

    # === é‡åŒ–å‚æ•° ===
    lines.append("## é‡åŒ–å‚æ•°")
    lines.append("")
    lines.append(f"- **æ–¹æ³•**: {meta['method']}")
    lines.append(f"- **èµ›é“**: Track {meta['track']}")
    for key in ("weight", "activation", "kv"):
        if key in config:
            params = config[key]
            lines.append(f"- **{key}**: {json.dumps(params, ensure_ascii=False)}")
    lines.append(f"- **Seed**: {config.get('common_hyperparams', {}).get('seed', 'N/A')}")
    lines.append("")

    # === PPL ç»“æœ ===
    ppl_results = results.get("ppl", {})
    if ppl_results:
        lines.append("## PPL ç»“æœ")
        lines.append("")
        lines.append("| æ•°æ®é›† | PPL | NLL |")
        lines.append("|--------|-----|-----|")
        for dataset, vals in ppl_results.items():
            if isinstance(vals, dict):
                ppl_val = vals.get("ppl", "N/A")
                nll_val = vals.get("nll", "N/A")
                lines.append(f"| {dataset} | {ppl_val} | {nll_val} |")
        lines.append("")

    # === lm-eval ç»“æœ ===
    lm_eval_results = results.get("lm_eval", {})
    if lm_eval_results and "error" not in lm_eval_results:
        lines.append("## lm-eval ä»»åŠ¡ç»“æœ")
        lines.append("")
        lines.append("| ä»»åŠ¡ | æŒ‡æ ‡ | åˆ†æ•° |")
        lines.append("|------|------|------|")
        for task_name, task_res in lm_eval_results.items():
            if task_name.startswith("_"):
                continue  # è·³è¿‡ _avg_accuracy ç­‰å…ƒå­—æ®µ
            if isinstance(task_res, dict):
                for metric, score in task_res.items():
                    lines.append(f"| {task_name} | {metric} | {score} |")
        avg_acc = lm_eval_results.get("_avg_accuracy")
        if avg_acc is not None:
            lines.append(f"\n**å¹³å‡å‡†ç¡®ç‡**: {avg_acc}")
        lines.append("")

    # === ç³»ç»ŸæŒ‡æ ‡ ===
    sys_metrics = results.get("system_metrics", {})
    if sys_metrics:
        lines.append("## ç³»ç»ŸæŒ‡æ ‡")
        lines.append("")
        vram = sys_metrics.get("vram_peak_mb")
        if vram:
            lines.append(f"- **VRAM å³°å€¼**: {vram} MB")
        eval_time = results.get("eval_time_seconds")
        if eval_time:
            lines.append(f"- **è¯„æµ‹æ€»è€—æ—¶**: {eval_time} ç§’")
        lines.append("")

    # === ç¯å¢ƒä¿¡æ¯ ===
    lines.append(format_env_info(env_info))
    lines.append("")

    return "\n".join(lines)
