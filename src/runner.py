# -*- coding: utf-8 -*-
"""
è¿è¡Œæ§åˆ¶å™¨

ç»„è£…å®Œæ•´çš„ "åŠ è½½æ¨¡å‹ â†’ é‡åŒ– â†’ è¯„æµ‹ â†’ ä¿å­˜ç»“æœ" æµæ°´çº¿ã€‚
æ˜¯ run_one.py çš„æ ¸å¿ƒå¼•æ“ã€‚

æµç¨‹:
1. åŠ è½½å¹¶åˆå¹¶é…ç½®
2. åŠ è½½æ¨¡å‹å’Œ tokenizer
3. å‡†å¤‡æ ¡å‡†æ•°æ®ï¼ˆå¦‚æœæ–¹æ³•éœ€è¦ï¼‰
4. è°ƒç”¨é‡åŒ–æ–¹æ³•
5. è¿è¡Œè¯„æµ‹
6. ä¿å­˜ç»“æœ
"""

import sys
import time
from pathlib import Path
from typing import Any

from src import config as cfg
from src import registry
from src.evaluator import evaluate
from src.result_writer import write_results
from src.env_info import collect_env_info


def run_experiment(
    model_name: str,
    method_name: str,
    track: str,
    cli_overrides: dict | None = None,
    output_dir: str | Path | None = None,
    dry_run: bool = False,
    cli_args_str: str = "",
    script_name: str = "",
) -> dict | None:
    """
    è¿è¡Œä¸€ä¸ªå®Œæ•´çš„å®éªŒï¼šåŠ è½½ â†’ é‡åŒ– â†’ è¯„æµ‹ â†’ ä¿å­˜ç»“æœã€‚

    å‚æ•°:
        model_name: æ¨¡å‹åç§°ï¼ˆå¯¹åº” configs/models/{name}.yamlï¼‰
        method_name: æ–¹æ³•åç§°ï¼ˆå¯¹åº” configs/methods/{name}.yamlï¼‰
        track: èµ›é“åç§°ï¼ˆA / B / Cï¼‰
        cli_overrides: CLI å‚æ•°è¦†ç›–å­—å…¸
        output_dir: ç»“æœè¾“å‡ºç›®å½•
        dry_run: åªæ‰“å°é…ç½®ä¸è¿è¡Œ
        cli_args_str: å®Œæ•´å‘½ä»¤è¡Œå­—ç¬¦ä¸²ï¼ˆç”¨äºè®°å½•ï¼‰
        script_name: è„šæœ¬åç§°ï¼ˆç”¨äºè®°å½•ï¼‰

    è¿”å›:
        dict: å®éªŒç»“æœå­—å…¸ï¼Œdry_run æ¨¡å¼è¿”å› None
    """
    project_root = cfg.get_project_root()

    # ================================================================
    # 1. åŠ è½½å¹¶åˆå¹¶é…ç½®
    # ================================================================
    print("=" * 60)
    print(f"ğŸ“‹ åŠ è½½é…ç½®: model={model_name}, method={method_name}, track={track}")
    print("=" * 60)

    global_config = cfg.load_global_config(project_root)
    model_config = cfg.load_model_config(model_name, project_root)
    method_config = cfg.load_method_config(method_name, project_root)
    track_config = cfg.load_track_config(track, project_root)

    # æ³¨å…¥ track åç§°åˆ°è¦†ç›–ä¸­
    overrides = {"track": track.upper()}
    if cli_overrides:
        overrides.update(cli_overrides)

    merged = cfg.merge_configs(global_config, model_config, method_config, track_config, overrides)
    merged = cfg.resolve_paths(merged, project_root)

    # éªŒè¯é…ç½®
    errors = cfg.validate_config(merged)
    if errors:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥:")
        for e in errors:
            print(f"   - {e}")
        return None

    # ================================================================
    # dry_run æ¨¡å¼ï¼šåªæ‰“å°é…ç½®
    # ================================================================
    if dry_run:
        print("\nğŸ” [DRY RUN] åˆå¹¶åçš„æœ€ç»ˆé…ç½®:")
        print("-" * 40)
        print(cfg.dump_config(merged))
        print("-" * 40)
        print("ğŸ” [DRY RUN] é…ç½®æ£€æŸ¥å®Œæ¯•ï¼Œä¸æ‰§è¡Œå®é™…è¿è¡Œ")
        return None

    # ================================================================
    # 2. æ£€æŸ¥æ–¹æ³•æ˜¯å¦æ”¯æŒè¯¥èµ›é“
    # ================================================================
    method_cls = registry.get(method_name)
    supported = getattr(method_cls, "supported_tracks", [])
    if track.upper() not in [t.upper() for t in supported]:
        print(f"âŒ æ–¹æ³• {method_name} ä¸æ”¯æŒ Track {track}")
        print(f"   æ”¯æŒçš„èµ›é“: {supported}")
        return None

    # ================================================================
    # 3. åŠ è½½æ¨¡å‹å’Œ tokenizer
    # ================================================================
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_config.get('model_id', 'unknown')}")
    model, tokenizer = _load_model(model_config, merged)

    # ================================================================
    # 4. å‡†å¤‡æ ¡å‡†æ•°æ®
    # ================================================================
    method_instance = method_cls(merged)
    calib_data = None
    if method_instance.requires_calibration():
        print(f"\nğŸ“¦ å‡†å¤‡æ ¡å‡†æ•°æ®")
        calib_data = _prepare_calibration_data(tokenizer, merged)

    # ================================================================
    # 5. æ‰§è¡Œé‡åŒ–
    # ================================================================
    print(f"\nâš¡ æ‰§è¡Œé‡åŒ–: {method_name}")
    start_quant = time.time()
    warnings = []

    try:
        model = method_instance.quantize(model, tokenizer, calib_data)
    except Exception as e:
        print(f"âŒ é‡åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    quant_time = time.time() - start_quant
    print(f"â±ï¸  é‡åŒ–è€—æ—¶: {quant_time:.1f} ç§’")

    # ================================================================
    # 6. è¿è¡Œè¯„æµ‹
    # ================================================================
    print(f"\nğŸ“Š å¼€å§‹è¯„æµ‹")
    results = evaluate(model, tokenizer, merged)
    results["quant_time_seconds"] = round(quant_time, 1)

    # ================================================================
    # 7. ä¿å­˜ç»“æœ
    # ================================================================
    if output_dir is None:
        output_dir = merged.get("paths", {}).get("results_root", "results")
    output_dir = Path(output_dir)

    md_path, json_path = write_results(
        results=results,
        config=merged,
        output_dir=output_dir,
        cli_args=cli_args_str,
        script_name=script_name,
        warnings=warnings,
    )

    return results


def _load_model(model_config: dict, merged_config: dict):
    """
    åŠ è½½ HuggingFace æ¨¡å‹å’Œ tokenizerã€‚

    å‚æ•°:
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        merged_config: åˆå¹¶åçš„å®Œæ•´é…ç½®

    è¿”å›:
        tuple: (model, tokenizer)
    """
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        model_id = model_config["model_id"]
        dtype_str = model_config.get("dtype", merged_config.get("common_hyperparams", {}).get("dtype", "float16"))
        dtype = getattr(torch, dtype_str, torch.float16)

        trust_remote_code = model_config.get("trust_remote_code", False)
        revision = model_config.get("revision", None)
        attn_impl = model_config.get("attn_implementation", None)
        cache_dir = merged_config.get("paths", {}).get("model_cache_dir", None)

        # åŠ è½½ tokenizer
        tokenizer_id = model_config.get("tokenizer_id") or model_id
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_id,
            trust_remote_code=trust_remote_code,
            revision=revision,
            cache_dir=cache_dir,
        )

        # åŠ è½½æ¨¡å‹
        model_kwargs = model_config.get("model_kwargs", {})
        load_kwargs = {
            "dtype": dtype,
            "device_map": "auto",
            "trust_remote_code": trust_remote_code,
            "revision": revision,
            "cache_dir": cache_dir,
            **model_kwargs,
        }
        if attn_impl:
            load_kwargs["attn_implementation"] = attn_impl

        model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)

        print(f"  âœ… æ¨¡å‹å·²åŠ è½½: {model_id} ({dtype_str})")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")

        return model, tokenizer

    except ImportError as e:
        print(f"âŒ æ¨¡å‹åŠ è½½ä¾èµ–ç¼ºå¤±: {e}")
        print("   è¯·å®‰è£…: pip install transformers torch")
        raise
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def _prepare_calibration_data(tokenizer: Any, config: dict) -> list:
    """
    å‡†å¤‡æ ¡å‡†æ•°æ®ã€‚

    ä»é…ç½®ä¸­è¯»å–æ ¡å‡†å‚æ•°ï¼Œä¸‹è½½/åŠ è½½æ ¡å‡†æ•°æ®é›†ï¼Œ
    tokenize å¹¶ pack æˆå›ºå®šé•¿åº¦çš„ token blocksã€‚

    å‚æ•°:
        tokenizer: tokenizer
        config: åˆå¹¶åçš„é…ç½®

    è¿”å›:
        list: æ ¡å‡†æ•°æ®åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å« input_idsï¼‰
    """
    calib_config = config.get("calibration", config.get("default_calibration", {}))
    dataset_name = calib_config.get("dataset", "wikitext2")
    num_samples = calib_config.get("num_samples", 128)
    seq_len = calib_config.get("seq_len", 2048)
    seed = calib_config.get("seed", 42)

    print(f"  æ ¡å‡†æ•°æ®é›†: {dataset_name}, æ ·æœ¬æ•°: {num_samples}, åºåˆ—é•¿åº¦: {seq_len}, seed: {seed}")

    try:
        import torch
        from datasets import load_dataset
        import random

        random.seed(seed)

        # åŠ è½½æ•°æ®é›†
        if dataset_name == "wikitext2":
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
            all_text = "\n\n".join([t for t in dataset["text"] if t.strip()])
        elif dataset_name == "c4":
            dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)
            texts = []
            for i, item in enumerate(dataset):
                if i >= num_samples * 2:  # å¤šå–ä¸€äº›ä»¥ç¡®ä¿å¤Ÿç”¨
                    break
                texts.append(item["text"])
            all_text = "\n\n".join(texts)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¡å‡†æ•°æ®é›†: {dataset_name}")

        # Tokenize
        encodings = tokenizer(all_text, return_tensors="pt")
        all_ids = encodings.input_ids[0]
        total_tokens = len(all_ids)

        print(f"  æ€» token æ•°: {total_tokens}")

        # éšæœºé‡‡æ ·å›ºå®šé•¿åº¦çš„ token blocks
        calib_data = []
        max_start = total_tokens - seq_len
        if max_start <= 0:
            print(f"âš ï¸ æ–‡æœ¬æ€» token æ•° ({total_tokens}) å°äº seq_len ({seq_len})")
            calib_data.append({"input_ids": all_ids[:seq_len].unsqueeze(0)})
        else:
            starts = random.sample(range(max_start), min(num_samples, max_start))
            for s in starts:
                chunk = all_ids[s:s + seq_len].unsqueeze(0)
                calib_data.append({"input_ids": chunk})

        print(f"  å·²ç”Ÿæˆ {len(calib_data)} ä¸ªæ ¡å‡†æ ·æœ¬")
        return calib_data

    except ImportError as e:
        print(f"âš ï¸  æ ¡å‡†æ•°æ®å‡†å¤‡ä¾èµ–ç¼ºå¤±: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸  æ ¡å‡†æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return []
