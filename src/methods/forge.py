# -*- coding: utf-8 -*-
"""
FORGE â€” åŠ¨æ€ç§©å…è®­ç»ƒæ½œç©ºé—´æ³¨æ„åŠ› KV Cache å‹ç¼©

Fast On-chip Reconstruction of Generative Embeddings (FORGE)

æ ¸å¿ƒæ€è·¯:
- å°† KV Cache æŒ‰ chunk_size åˆ†å—ï¼Œå¯¹æ¯å—åš SVD åˆ†è§£
- æ ¹æ®å¥‡å¼‚å€¼èƒ½é‡è°±åŠ¨æ€å†³å®šæ¯å—ä¿ç•™çš„ç§© (rank)
- æ¨ç†æ—¶ç”¨ U @ diag(S) @ V^T é‡æ„ KVï¼Œç”¨å®Œå³ä¸¢
- ç”¨é—²ç½®ç®—åŠ›æ¢æ˜¾å­˜å¸¦å®½ï¼Œçº¯åè®­ç»ƒ (Post-Training) æ–¹æ¡ˆ

ä¿®å¤è¯´æ˜ (2026-02-17):
  ä» monkey-patch attention forward è¿ç§»åˆ°è‡ªå®šä¹‰ CacheLayerã€‚
"""

import torch
import torch.nn as nn
from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any
from transformers.cache_utils import DynamicLayer, DynamicCache


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SVD å‹ç¼©å·¥å…·å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _compute_dynamic_rank(singular_values: torch.Tensor, energy_threshold: float,
                          min_rank: int, max_rank: int) -> torch.Tensor:
    """
    æ ¹æ®å¥‡å¼‚å€¼èƒ½é‡è°±åŠ¨æ€è®¡ç®—æœ€ä¼˜ç§©ã€‚

    é€šè¿‡ç´¯ç§¯èƒ½é‡æ¯”åˆ¤æ–­ä¿ç•™å¤šå°‘ä¸»æˆåˆ†:
    retained_energy = cumsum(sigma^2) / sum(sigma^2)
    """
    energy = singular_values ** 2
    cumulative_energy = torch.cumsum(energy, dim=-1)
    total_energy = cumulative_energy[..., -1:]
    energy_ratio = cumulative_energy / total_energy.clamp(min=1e-10)

    rank = (energy_ratio >= energy_threshold).long().argmax(dim=-1) + 1
    rank = rank.clamp(min=min_rank, max=max_rank)
    return rank


def _svd_compress(tensor: torch.Tensor, energy_threshold: float,
                  min_rank: int, max_rank: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:
    """
    å¯¹ä¸€ä¸ª chunk åš truncated SVD å‹ç¼©ã€‚

    è¿”å›: (U_trunc, S_trunc, Vh_trunc, rank)
    """
    U, S, Vh = torch.linalg.svd(tensor.float(), full_matrices=False)
    rank = _compute_dynamic_rank(S, energy_threshold, min_rank, max_rank)
    r = rank.max().item()

    U_trunc = U[:, :, :, :r].to(tensor.dtype)
    S_trunc = S[:, :, :r].to(tensor.dtype)
    Vh_trunc = Vh[:, :, :r, :].to(tensor.dtype)
    return U_trunc, S_trunc, Vh_trunc, r


def _svd_reconstruct(compressed: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:
    """ä» (U, S, Vh) ä¸‰å…ƒç»„é‡æ„ä¸€ä¸ª chunkã€‚"""
    U, S, Vh = compressed
    return torch.matmul(U * S.unsqueeze(-2), Vh)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ForgeCacheLayer â€” å­ç±»åŒ– DynamicLayer
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ForgeCacheLayer(DynamicLayer):
    """
    FORGE å‹ç¼©çš„ Cache Layerã€‚

    ç»§æ‰¿ DynamicLayerï¼Œè¦†å†™ update() æ¥æ‹¦æˆª KV states å¹¶ç”¨ SVD å‹ç¼©ã€‚
    å·²å‡‘æ»¡ chunk_size çš„éƒ¨åˆ†ç”¨ (U, S, Vh) å­˜å‚¨ï¼Œæ®‹ä½™ä¿æŒåŸå§‹ç²¾åº¦ã€‚
    """

    def __init__(self, chunk_size: int = 64, energy_threshold: float = 0.95,
                 min_rank: int = 2, max_rank: int = 32, layer_idx: int = 0):
        super().__init__()
        self.chunk_size = chunk_size
        self.energy_threshold = energy_threshold
        self.min_rank = min_rank
        self.max_rank = max_rank
        self.layer_idx = layer_idx
        self.cumulative_length = 0

        # å‹ç¼©å­˜å‚¨
        self._compressed_key_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self._compressed_value_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self._residual_key: torch.Tensor | None = None
        self._residual_value: torch.Tensor | None = None

        # ç»Ÿè®¡
        self._compress_count = 0
        self._ranks_used: list[int] = []

    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        cache_kwargs: dict[str, Any] | None = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        æ‹¦æˆª KV æ›´æ–°ï¼Œç”¨ SVD æŒ‰ chunk å‹ç¼©ã€‚

        æµç¨‹:
        1. æ‹¼æ¥æ®‹ä½™ + æ–° token
        2. å‡‘æ»¡ chunk_size çš„éƒ¨åˆ†åš SVD å‹ç¼©
        3. å‰©ä½™ç»§ç»­ä¿ç•™ä¸ºæ®‹ä½™
        4. è¿”å›å®Œæ•´é‡æ„çš„ KV (å‹ç¼© chunks é‡æ„ + æ®‹ä½™)
        """
        if not self.is_initialized:
            self.lazy_initialization(key_states, value_states)

        self.cumulative_length += key_states.shape[-2]

        # Step 1: æ‹¼æ¥æ®‹ä½™
        if self._residual_key is not None and self._residual_key.numel() > 0:
            key_combined = torch.cat([self._residual_key, key_states], dim=-2)
            value_combined = torch.cat([self._residual_value, value_states], dim=-2)
        else:
            key_combined = key_states
            value_combined = value_states

        seq_len = key_combined.shape[-2]

        # Step 2: åˆ†å—å‹ç¼©
        n_full_chunks = seq_len // self.chunk_size
        compressed_len = n_full_chunks * self.chunk_size

        if n_full_chunks > 0:
            for i in range(n_full_chunks):
                start = i * self.chunk_size
                end = start + self.chunk_size

                k_chunk = key_combined[:, :, start:end, :]
                v_chunk = value_combined[:, :, start:end, :]

                ku, ks, kvh, kr = _svd_compress(k_chunk, self.energy_threshold,
                                                 self.min_rank, self.max_rank)
                vu, vs, vvh, vr = _svd_compress(v_chunk, self.energy_threshold,
                                                 self.min_rank, self.max_rank)

                self._compressed_key_chunks.append((ku, ks, kvh))
                self._compressed_value_chunks.append((vu, vs, vvh))
                self._ranks_used.extend([kr, vr])
                self._compress_count += 1

        # Step 3: ä¿å­˜æ®‹ä½™
        if compressed_len < seq_len:
            self._residual_key = key_combined[:, :, compressed_len:, :].contiguous()
            self._residual_value = value_combined[:, :, compressed_len:, :].contiguous()
        else:
            self._residual_key = None
            self._residual_value = None

        # æ¸…ç©ºçˆ¶ç±»çš„ self.keys/values
        self.keys = torch.tensor([], dtype=key_states.dtype, device=key_states.device)
        self.values = torch.tensor([], dtype=key_states.dtype, device=key_states.device)

        # Step 4: é‡æ„å®Œæ•´ KV
        key_parts = [_svd_reconstruct(c) for c in self._compressed_key_chunks]
        value_parts = [_svd_reconstruct(c) for c in self._compressed_value_chunks]

        if self._residual_key is not None and self._residual_key.numel() > 0:
            key_parts.append(self._residual_key)
            value_parts.append(self._residual_value)

        if not key_parts:
            empty = torch.tensor([], dtype=key_states.dtype, device=key_states.device)
            return empty, empty

        full_key = torch.cat(key_parts, dim=-2)
        full_value = torch.cat(value_parts, dim=-2)

        return full_key, full_value

    def get_seq_length(self) -> int:
        return self.cumulative_length


class ForgeQuantizedCache(DynamicCache):
    """ç”¨ ForgeCacheLayer æ›¿ä»£ DynamicLayer çš„ DynamicCacheã€‚"""

    def __init__(self, chunk_size: int = 64, energy_threshold: float = 0.95,
                 min_rank: int = 2, max_rank: int = 32,
                 num_layers: int = 32, **kwargs):
        layers = [
            ForgeCacheLayer(
                chunk_size=chunk_size,
                energy_threshold=energy_threshold,
                min_rank=min_rank,
                max_rank=max_rank,
                layer_idx=i,
            )
            for i in range(num_layers)
        ]
        from transformers.cache_utils import Cache
        Cache.__init__(self, layers=layers)

    def get_compress_stats(self) -> dict:
        """è¿”å›å‹ç¼©ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        all_ranks = []
        for layer in self.layers:
            if isinstance(layer, ForgeCacheLayer):
                all_ranks.extend(layer._ranks_used)
        if not all_ranks:
            return {"avg_rank": 0, "num_chunks": 0}
        return {
            "avg_rank": round(sum(all_ranks) / len(all_ranks), 1),
            "min_rank": min(all_ranks),
            "max_rank": max(all_ranks),
            "num_chunks": len(all_ranks) // 2,
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ³¨å…¥æœºåˆ¶
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _inject_forge_cache(model: nn.Module, forge_config: dict):
    """Monkey-patch model.generate() ä½¿å…¶ä½¿ç”¨ ForgeQuantizedCacheã€‚"""
    chunk_size = forge_config["chunk_size"]
    energy_threshold = forge_config["energy_threshold"]
    min_rank = forge_config["min_rank"]
    max_rank = forge_config["max_rank"]
    num_layers = model.config.num_hidden_layers

    original_generate = model.generate

    def patched_generate(*args, **kwargs):
        if "past_key_values" not in kwargs or kwargs["past_key_values"] is None:
            cache = ForgeQuantizedCache(
                chunk_size=chunk_size,
                energy_threshold=energy_threshold,
                min_rank=min_rank,
                max_rank=max_rank,
                num_layers=num_layers,
            )
            kwargs["past_key_values"] = cache

        result = original_generate(*args, **kwargs)

        if not hasattr(model, "_forge_stats_printed"):
            model._forge_stats_printed = True
            if isinstance(kwargs.get("past_key_values"), ForgeQuantizedCache):
                stats = kwargs["past_key_values"].get_compress_stats()
                if stats["num_chunks"] > 0:
                    print(f"  ğŸ“Š FORGE å‹ç¼©ç¡®è®¤: {stats['num_chunks']} chunks, "
                          f"avg_rank={stats['avg_rank']}, range=[{stats['min_rank']},{stats['max_rank']}]")
                else:
                    print("  âš ï¸ FORGE: å‹ç¼©æœªè§¦å‘ (åºåˆ—å¯èƒ½å¤ªçŸ­)")

        return result

    model.generate = patched_generate
    print(f"  âœ… FORGE Cache æ³¨å…¥å®Œæˆ: {num_layers} å±‚, "
          f"chunk={chunk_size}, energy={energy_threshold}")


@register("forge")
class ForgeMethod(BaseQuantMethod):
    """FORGE â€” åŠ¨æ€ç§©å…è®­ç»ƒ KV Cache å‹ç¼©ã€‚é€šè¿‡è‡ªå®šä¹‰ DynamicCache å®ç°ã€‚"""

    supported_tracks = ["C"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        kv_config = self.config.get("kv", {})
        chunk_size = kv_config.get("chunk_size", 64)
        energy_threshold = kv_config.get("energy_threshold", 0.95)
        min_rank = kv_config.get("min_rank", 2)
        max_rank = kv_config.get("max_rank", 32)

        print(f"ğŸ“‹ FORGE: chunk_size={chunk_size}, energy_threshold={energy_threshold}")
        print(f"ğŸ“‹ FORGE: rank_range=[{min_rank}, {max_rank}]")
        print(f"ğŸ“‹ FORGE: å…æ ¡å‡†ï¼ŒåŠ¨æ€ç§© SVD å‹ç¼©")

        forge_config = {
            "chunk_size": chunk_size,
            "energy_threshold": energy_threshold,
            "min_rank": min_rank,
            "max_rank": max_rank,
        }

        _inject_forge_cache(model, forge_config)
        return model
