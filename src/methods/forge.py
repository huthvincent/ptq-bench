# -*- coding: utf-8 -*-
"""
FORGE â€” åŠ¨æ€ç§©å…è®­ç»ƒæ½œç©ºé—´æ³¨æ„åŠ› KV Cache å‹ç¼©

Fast On-chip Reconstruction of Generative Embeddings (FORGE)

æ ¸å¿ƒæ€è·¯:
- å°† KV Cache æŒ‰ chunk_size åˆ†å—ï¼Œå¯¹æ¯å—åš SVD åˆ†è§£
- æ ¹æ®å¥‡å¼‚å€¼èƒ½é‡è°±åŠ¨æ€å†³å®šæ¯å—ä¿ç•™çš„ç§© (rank)
- æ¨ç†æ—¶ç”¨ U @ diag(S) @ V^T é‡æ„ KVï¼Œç”¨å®Œå³ä¸¢
- ç”¨é—²ç½®ç®—åŠ›æ¢æ˜¾å­˜å¸¦å®½ï¼Œçº¯åè®­ç»ƒ (Post-Training) æ–¹æ¡ˆ
"""

import torch
import torch.nn as nn
from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any


class ForgeKVCache:
    """
    FORGE KV Cache ç®¡ç†å™¨ã€‚

    å°† KV æŒ‰ chunk åˆ†å—å¹¶ç”¨ truncated SVD å‹ç¼©å­˜å‚¨ã€‚
    æ¯ä¸ª chunk åªä¿å­˜ (U, S, V) ä¸‰å…ƒç»„ï¼Œç§©ç”±ä¿¡æ¯ä¸°å¯Œåº¦åŠ¨æ€å†³å®šã€‚
    """

    def __init__(self, chunk_size: int = 64, energy_threshold: float = 0.95,
                 min_rank: int = 2, max_rank: int = 32):
        """
        åˆå§‹åŒ– FORGE KV Cacheã€‚

        å‚æ•°:
            chunk_size: æ¯ä¸ªåˆ†å—çš„ token æ•°
            energy_threshold: SVD èƒ½é‡ä¿ç•™é˜ˆå€¼ (0~1)
            min_rank: æ¯ä¸ª chunk æœ€å°‘ä¿ç•™çš„ç§©
            max_rank: æ¯ä¸ª chunk æœ€å¤šä¿ç•™çš„ç§©
        """
        self.chunk_size = chunk_size
        self.energy_threshold = energy_threshold
        self.min_rank = min_rank
        self.max_rank = max_rank

        # å·²å‹ç¼©çš„ chunks: list of (U, S, V) for key å’Œ value
        self.compressed_key_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self.compressed_value_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []

        # å°šæœªå‡‘æ»¡ä¸€ä¸ª chunk çš„æ®‹ä½™ KV (åŸå§‹æ ¼å¼)
        self.residual_key: torch.Tensor | None = None
        self.residual_value: torch.Tensor | None = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_tokens_compressed = 0
        self.total_ranks_used: list[int] = []

    def update(self, key: torch.Tensor, value: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        æ¥æ”¶æ–°çš„ KV å¹¶æ›´æ–°ç¼“å­˜ã€‚

        å°†æ–° KV ä¸æ®‹ä½™æ‹¼æ¥ï¼Œå‡‘æ»¡ chunk çš„éƒ¨åˆ†å‹ç¼©å­˜å‚¨ï¼Œ
        å‰©ä½™çš„ç»§ç»­ä¿ç•™ä¸ºæ®‹ä½™ã€‚è¿”å›é‡æ„åçš„å®Œæ•´ KV ä¾› Attention ä½¿ç”¨ã€‚

        å‚æ•°:
            key: æ–°çš„ Key tensor, å½¢çŠ¶ [B, H, S_new, D]
            value: æ–°çš„ Value tensor, å½¢çŠ¶ [B, H, S_new, D]

        è¿”å›:
            tuple: (full_key, full_value) é‡æ„åçš„å®Œæ•´ KV
        """
        # æ‹¼æ¥æ®‹ä½™
        if self.residual_key is not None:
            key = torch.cat([self.residual_key, key], dim=2)
            value = torch.cat([self.residual_value, value], dim=2)

        seq_len = key.size(2)

        # å°†å‡‘æ»¡ chunk çš„éƒ¨åˆ†å‹ç¼©
        n_full_chunks = seq_len // self.chunk_size
        compressed_len = n_full_chunks * self.chunk_size

        if n_full_chunks > 0:
            for i in range(n_full_chunks):
                start = i * self.chunk_size
                end = start + self.chunk_size

                k_chunk = key[:, :, start:end, :]   # [B, H, chunk_size, D]
                v_chunk = value[:, :, start:end, :]

                k_compressed = self._svd_compress(k_chunk)
                v_compressed = self._svd_compress(v_chunk)

                self.compressed_key_chunks.append(k_compressed)
                self.compressed_value_chunks.append(v_compressed)
                self.total_tokens_compressed += self.chunk_size

        # ä¿å­˜æ®‹ä½™
        if compressed_len < seq_len:
            self.residual_key = key[:, :, compressed_len:, :].contiguous()
            self.residual_value = value[:, :, compressed_len:, :].contiguous()
        else:
            self.residual_key = None
            self.residual_value = None

        # é‡æ„å®Œæ•´ KV ä¾›æœ¬æ¬¡ Attention ä½¿ç”¨
        full_key = self._reconstruct_all_keys()
        full_value = self._reconstruct_all_values()

        return full_key, full_value

    def _svd_compress(self, tensor: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å¯¹ä¸€ä¸ª chunk åš truncated SVD å‹ç¼©ã€‚

        å‚æ•°:
            tensor: å½¢çŠ¶ [B, H, chunk_size, D]

        è¿”å›:
            tuple: (U, S, V) æˆªæ–­åçš„ä¸‰å…ƒç»„
                U: [B, H, chunk_size, r]
                S: [B, H, r]
                V: [B, H, r, D]
        """
        B, H, T, D = tensor.shape

        # SVD: tensor = U @ diag(S) @ V^T
        # torch.linalg.svd è¿”å› U: [B,H,T,K], S: [B,H,K], Vh: [B,H,K,D], K=min(T,D)
        U, S, Vh = torch.linalg.svd(tensor.float(), full_matrices=False)

        # åŠ¨æ€è®¡ç®—ç§©
        rank = _compute_dynamic_rank(S, self.energy_threshold, self.min_rank, self.max_rank)

        # å–æ‰€æœ‰ head çš„æœ€å¤§ç§© (ç®€åŒ–å®ç°ï¼Œä¿è¯å½¢çŠ¶ä¸€è‡´)
        r = rank.max().item()
        self.total_ranks_used.append(r)

        # æˆªæ–­
        U_trunc = U[:, :, :, :r].to(tensor.dtype)    # [B, H, T, r]
        S_trunc = S[:, :, :r].to(tensor.dtype)        # [B, H, r]
        Vh_trunc = Vh[:, :, :r, :].to(tensor.dtype)   # [B, H, r, D]

        return (U_trunc, S_trunc, Vh_trunc)

    def _reconstruct_chunk(self, compressed: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:
        """
        ä» (U, S, V) ä¸‰å…ƒç»„é‡æ„ä¸€ä¸ª chunkã€‚

        å‚æ•°:
            compressed: (U, S, Vh) ä¸‰å…ƒç»„

        è¿”å›:
            torch.Tensor: é‡æ„çš„ tensor, å½¢çŠ¶ [B, H, chunk_size, D]
        """
        U, S, Vh = compressed
        # reconstructed = U @ diag(S) @ Vh
        # U: [B, H, T, r], S: [B, H, r], Vh: [B, H, r, D]
        return torch.matmul(U * S.unsqueeze(-2), Vh)

    def _reconstruct_all_keys(self) -> torch.Tensor:
        """é‡æ„æ‰€æœ‰ Key (å‹ç¼© chunks + æ®‹ä½™)ã€‚"""
        parts = []
        for compressed in self.compressed_key_chunks:
            parts.append(self._reconstruct_chunk(compressed))
        if self.residual_key is not None:
            parts.append(self.residual_key)
        if not parts:
            return torch.empty(0)
        return torch.cat(parts, dim=2)

    def _reconstruct_all_values(self) -> torch.Tensor:
        """é‡æ„æ‰€æœ‰ Value (å‹ç¼© chunks + æ®‹ä½™)ã€‚"""
        parts = []
        for compressed in self.compressed_value_chunks:
            parts.append(self._reconstruct_chunk(compressed))
        if self.residual_value is not None:
            parts.append(self.residual_value)
        if not parts:
            return torch.empty(0)
        return torch.cat(parts, dim=2)

    def get_seq_len(self) -> int:
        """è¿”å›å½“å‰ç¼“å­˜çš„æ€» token æ•°ã€‚"""
        n = len(self.compressed_key_chunks) * self.chunk_size
        if self.residual_key is not None:
            n += self.residual_key.size(2)
        return n

    def get_memory_stats(self) -> dict:
        """
        è¿”å›å‹ç¼©ç»Ÿè®¡ä¿¡æ¯ã€‚

        è¿”å›:
            dict: åŒ…å«å¹³å‡ç§©ã€å‹ç¼©æ¯”ç­‰ä¿¡æ¯
        """
        if not self.total_ranks_used:
            return {"avg_rank": 0, "compression_ratio": 1.0, "num_chunks": 0}

        avg_rank = sum(self.total_ranks_used) / len(self.total_ranks_used)
        # åŸå§‹å­˜å‚¨: chunk_size * D per chunk
        # å‹ç¼©å­˜å‚¨: chunk_size * r + r + r * D â‰ˆ r * (chunk_size + D)
        # å‡è®¾ D â‰ˆ head_dim (é€šå¸¸ 128)
        # å‹ç¼©æ¯” â‰ˆ (chunk_size * D) / (r * (chunk_size + D))
        D_est = 128  # å…¸å‹ head_dim
        original = self.chunk_size * D_est
        compressed = avg_rank * (self.chunk_size + D_est)
        ratio = original / compressed if compressed > 0 else 1.0

        return {
            "avg_rank": round(avg_rank, 1),
            "min_rank_used": min(self.total_ranks_used),
            "max_rank_used": max(self.total_ranks_used),
            "num_chunks": len(self.total_ranks_used) // 2,  # key å’Œ value å„ä¸€åŠ
            "compression_ratio": round(ratio, 2),
            "total_tokens_compressed": self.total_tokens_compressed,
        }


def _compute_dynamic_rank(singular_values: torch.Tensor, energy_threshold: float,
                          min_rank: int, max_rank: int) -> torch.Tensor:
    """
    æ ¹æ®å¥‡å¼‚å€¼èƒ½é‡è°±åŠ¨æ€è®¡ç®—æœ€ä¼˜ç§©ã€‚

    é€šè¿‡ç´¯ç§¯èƒ½é‡æ¯”åˆ¤æ–­ä¿ç•™å¤šå°‘ä¸»æˆåˆ†:
    retained_energy = cumsum(sigma^2) / sum(sigma^2)
    æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= energy_threshold çš„ä½ç½®å³ä¸ºç§©ã€‚

    å‚æ•°:
        singular_values: å¥‡å¼‚å€¼, å½¢çŠ¶ [B, H, K]
        energy_threshold: èƒ½é‡ä¿ç•™é˜ˆå€¼ (0~1)
        min_rank: æœ€å°ç§©
        max_rank: æœ€å¤§ç§©

    è¿”å›:
        torch.Tensor: æ¯ä¸ª (batch, head) çš„æœ€ä¼˜ç§©, å½¢çŠ¶ [B, H]
    """
    # è®¡ç®—èƒ½é‡: sigma^2
    energy = singular_values ** 2

    # ç´¯ç§¯èƒ½é‡å æ¯”
    cumulative_energy = torch.cumsum(energy, dim=-1)
    total_energy = cumulative_energy[..., -1:]  # [B, H, 1]
    energy_ratio = cumulative_energy / total_energy.clamp(min=1e-10)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= threshold çš„ä½ç½®
    # argmax åœ¨ bool tensor ä¸Šè¿”å›ç¬¬ä¸€ä¸ª True çš„ä½ç½®
    rank = (energy_ratio >= energy_threshold).long().argmax(dim=-1) + 1  # [B, H]

    # è¾¹ç•Œä¿æŠ¤
    rank = rank.clamp(min=min_rank, max=max_rank)

    return rank


def _patch_attention_layers(model: nn.Module, forge_config: dict) -> list[ForgeKVCache]:
    """
    Monkey-patch æ¨¡å‹çš„ Attention å±‚ï¼Œæ¤å…¥ FORGE KV Cacheã€‚

    éå†æ¨¡å‹æ‰¾åˆ°æ‰€æœ‰ Attention å±‚ï¼Œæ›¿æ¢å…¶ forward æ–¹æ³•ï¼Œ
    ä½¿å…¶ä½¿ç”¨ ForgeKVCache ç®¡ç† KV è€ŒéåŸå§‹çš„ past_key_valuesã€‚

    å…¼å®¹ LlamaAttention / Qwen2Attention ç­‰æ ‡å‡† HuggingFace æ¶æ„ã€‚

    å‚æ•°:
        model: HuggingFace æ¨¡å‹
        forge_config: FORGE é…ç½®å­—å…¸ (chunk_size, energy_threshold, etc.)

    è¿”å›:
        list[ForgeKVCache]: æ‰€æœ‰ Attention å±‚çš„ FORGE cache å®ä¾‹åˆ—è¡¨
    """
    chunk_size = forge_config.get("chunk_size", 64)
    energy_threshold = forge_config.get("energy_threshold", 0.95)
    min_rank = forge_config.get("min_rank", 2)
    max_rank = forge_config.get("max_rank", 32)

    caches = []
    patched_count = 0

    for name, module in model.named_modules():
        # åŒ¹é…å¸¸è§ Attention å±‚åç§°
        module_type = type(module).__name__
        if not any(kw in module_type for kw in ("Attention",)):
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰ k_proj / v_proj (æ ‡å‡† HF Attention ç‰¹å¾)
        has_kv_proj = hasattr(module, "k_proj") and hasattr(module, "v_proj")
        if not has_kv_proj:
            continue

        # åˆ›å»ºè¯¥å±‚çš„ FORGE cache
        cache = ForgeKVCache(
            chunk_size=chunk_size,
            energy_threshold=energy_threshold,
            min_rank=min_rank,
            max_rank=max_rank,
        )
        caches.append(cache)

        # ä¿å­˜åŸå§‹ forward
        original_forward = module.forward

        # åˆ›å»º patched forward â€” é€šè¿‡é—­åŒ…æ•è· cache å’Œ original_forward
        def make_patched_forward(orig_fwd, forge_cache, attn_module):
            """
            æ„å»º patched forward å‡½æ•°ã€‚

            é‡‡ç”¨"å…ˆæ­£å¸¸è·‘ â†’ å†å‹ç¼© KV"çš„ç­–ç•¥:
            1. è°ƒç”¨åŸå§‹ forward è·å¾— attention è¾“å‡ºå’Œ KV
            2. å°† KV é€å…¥ ForgeKVCache å‹ç¼©
            3. ç”¨é‡æ„çš„ KV æ›¿æ¢ past_key_values
            """
            def patched_forward(*args, **kwargs):
                # è°ƒç”¨åŸå§‹ forward
                outputs = orig_fwd(*args, **kwargs)

                # outputs é€šå¸¸æ˜¯ (attn_output, attn_weights, past_key_values)
                # æˆ–è€… (attn_output, None, past_key_values)
                if isinstance(outputs, tuple) and len(outputs) >= 3:
                    attn_output = outputs[0]
                    attn_weights = outputs[1]
                    past_kv = outputs[2]

                    # past_kv é€šå¸¸æ˜¯ (key, value) tuple
                    if isinstance(past_kv, tuple) and len(past_kv) == 2:
                        key_states, value_states = past_kv

                        # æ¸…ç©ºæ—§ cache (å› ä¸ºåŸå§‹ forward å·²ç»ç´¯ç§¯äº†)
                        # æˆ‘ä»¬æ¯æ¬¡ä»å¤´å‹ç¼©å®Œæ•´ KV
                        forge_cache.compressed_key_chunks.clear()
                        forge_cache.compressed_value_chunks.clear()
                        forge_cache.residual_key = None
                        forge_cache.residual_value = None
                        forge_cache.total_tokens_compressed = 0

                        # å¯¹å®Œæ•´ KV åš FORGE å‹ç¼©
                        compressed_key, compressed_value = forge_cache.update(
                            key_states, value_states
                        )

                        # ç”¨å‹ç¼©-é‡æ„åçš„ KV æ›¿æ¢
                        outputs = (attn_output, attn_weights, (compressed_key, compressed_value))

                return outputs

            return patched_forward

        module.forward = make_patched_forward(original_forward, cache, module)
        patched_count += 1
        print(f"  âš¡ Patched: {name} ({module_type})")

    print(f"ğŸ“‹ FORGE: å…± patch äº† {patched_count} ä¸ª Attention å±‚")
    return caches


@register("forge")
class ForgeMethod(BaseQuantMethod):
    """
    FORGE é‡åŒ–æ–¹æ³• â€” åŠ¨æ€ç§©å…è®­ç»ƒ KV Cache å‹ç¼©ã€‚

    é€šè¿‡ monkey-patch Attention å±‚ï¼Œæ¤å…¥åŸºäº SVD çš„åŠ¨æ€ç§©
    KV Cache å‹ç¼©æœºåˆ¶ã€‚ä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼Œçº¯åè®­ç»ƒæ–¹æ¡ˆã€‚
    """

    supported_tracks = ["C"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        """
        æ‰§è¡Œ FORGE "é‡åŒ–"ï¼ˆå®é™…æ˜¯å®‰è£… KV Cache å‹ç¼©å™¨ï¼‰ã€‚

        æ­¥éª¤:
        1. ä» config è¯»å– FORGE å‚æ•°
        2. Monkey-patch æ‰€æœ‰ Attention å±‚çš„ forward
        3. è¿”å› patched åçš„æ¨¡å‹

        å‚æ•°:
            model: åŸå§‹ FP16/BF16 æ¨¡å‹
            tokenizer: tokenizerï¼ˆæœªä½¿ç”¨ï¼‰
            calib_data: æ ¡å‡†æ•°æ®ï¼ˆFORGE ä¸ä½¿ç”¨ï¼‰

        è¿”å›:
            Any: å®‰è£…äº† FORGE KV Cache çš„æ¨¡å‹
        """
        kv_config = self.config.get("kv", {})
        chunk_size = kv_config.get("chunk_size", 64)
        energy_threshold = kv_config.get("energy_threshold", 0.95)
        min_rank = kv_config.get("min_rank", 2)
        max_rank = kv_config.get("max_rank", 32)

        print(f"ğŸ“‹ FORGE: chunk_size={chunk_size}, energy_threshold={energy_threshold}")
        print(f"ğŸ“‹ FORGE: rank_range=[{min_rank}, {max_rank}]")
        print(f"ğŸ“‹ FORGE: å…æ ¡å‡†ï¼ŒåŠ¨æ€ç§© SVD å‹ç¼©")

        forge_config = {
            "chunk_size": chunk_size,
            "energy_threshold": energy_threshold,
            "min_rank": min_rank,
            "max_rank": max_rank,
        }

        # Patch Attention å±‚
        caches = _patch_attention_layers(model, forge_config)

        if not caches:
            print("âš ï¸  æœªæ‰¾åˆ°å¯ patch çš„ Attention å±‚ï¼ŒFORGE æœªç”Ÿæ•ˆ")
        else:
            print(f"âœ… FORGE å®‰è£…å®Œæˆ: {len(caches)} ä¸ª Attention å±‚å·²å¯ç”¨åŠ¨æ€ç§© KV å‹ç¼©")

        # å°† caches æŒ‚åœ¨æ¨¡å‹ä¸Šï¼Œæ–¹ä¾¿åç»­è·å–ç»Ÿè®¡ä¿¡æ¯
        model._forge_caches = caches

        return model
