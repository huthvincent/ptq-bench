# -*- coding: utf-8 -*-
"""
GPTQ â€” åŸºäº Hessian ä¿¡æ¯çš„æƒé‡é‡åŒ–æ–¹æ³•

åŠ è½½ HuggingFace Hub ä¸Šçš„é¢„é‡åŒ– GPTQ æ¨¡å‹è¿›è¡Œè¯„æµ‹ã€‚
ç”±äº auto_gptq ä¸ transformers 4.52 å­˜åœ¨å…¼å®¹æ€§é—®é¢˜
ï¼ˆno_init_weights å¯¼å…¥å¤±è´¥ï¼‰ï¼Œå› æ­¤ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹ã€‚
"""

from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any


@register("gptq")
class GPTQMethod(BaseQuantMethod):
    """
    GPTQ é‡åŒ–æ–¹æ³• wrapperã€‚

    åŠ è½½ HuggingFace Hub ä¸Šçš„é¢„é‡åŒ– GPTQ æ¨¡å‹ã€‚
    """

    supported_tracks = ["A"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        """
        åŠ è½½é¢„é‡åŒ–çš„ GPTQ æ¨¡å‹ã€‚

        å‚æ•°:
            model: åŸå§‹æ¨¡å‹ï¼ˆå°†è¢«é‡Šæ”¾ï¼‰
            tokenizer: tokenizer
            calib_data: æ ¡å‡†æ•°æ®ï¼ˆé¢„é‡åŒ–æ¨¡å‹ä¸éœ€è¦ï¼‰

        è¿”å›:
            Any: é¢„é‡åŒ–çš„ GPTQ æ¨¡å‹
        """
        import torch

        model_id = self.config.get("model", {}).get("model_id", "")
        pretrained_quant = self.config.get("model", {}).get("pretrained_quant_models", {})
        gptq_model_id = self.config.get("weight", {}).get("pretrained_model_id", None)
        if gptq_model_id is None:
            gptq_model_id = pretrained_quant.get("gptq", model_id + "-GPTQ-Int4")

        cache_dir = self.config.get("paths", {}).get("model_cache_dir", None)
        trust_remote_code = self.config.get("model", {}).get("trust_remote_code", False)

        print(f"ğŸ“‹ GPTQ: åŠ è½½é¢„é‡åŒ–æ¨¡å‹: {gptq_model_id}")

        # é‡Šæ”¾åŸå§‹æ¨¡å‹
        del model
        torch.cuda.empty_cache()

        from transformers import AutoModelForCausalLM, AutoTokenizer

        quant_model = AutoModelForCausalLM.from_pretrained(
            gptq_model_id,
            device_map="auto",
            trust_remote_code=trust_remote_code,
            cache_dir=cache_dir,
        )

        # é‡æ–°åŠ è½½ tokenizer
        new_tokenizer = AutoTokenizer.from_pretrained(
            gptq_model_id,
            trust_remote_code=trust_remote_code,
            cache_dir=cache_dir,
        )

        self._new_tokenizer = new_tokenizer

        print(f"âœ… GPTQ é¢„é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ: {gptq_model_id}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in quant_model.parameters()) / 1e9:.2f}B")
        return quant_model
