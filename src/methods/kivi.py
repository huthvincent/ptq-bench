# -*- coding: utf-8 -*-
"""
KIVI â€” Tuning-free Asymmetric 2-bit KV Cache Quantization

æ ¸å¿ƒæ€è·¯:
- Key Cache: æŒ‰ **channel ç»´åº¦** åˆ†ç»„é‡åŒ– (per-channel)
  â†’ å› ä¸º Key çš„å¼‚å¸¸å€¼é›†ä¸­åœ¨å›ºå®š channel
- Value Cache: æŒ‰ **token ç»´åº¦** åˆ†ç»„é‡åŒ– (per-token)
  â†’ Value æ— æ˜æ˜¾ channel å¼‚å¸¸å€¼æ¨¡å¼
- æœªå‡‘æ»¡ group çš„æ®‹ä½™ token ä¿æŒ FP16
- å…æ ¡å‡†ï¼Œè¿è¡Œæ—¶åœ¨çº¿é‡åŒ–

å‚è€ƒ: Zirui Liu et al., "KIVI: A Tuning-Free Asymmetric 2bit Quantization
for KV Cache", ICML 2024.
"""

import torch
import torch.nn as nn
from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any


def _asymmetric_quantize(tensor: torch.Tensor, bits: int, dim: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    æ²¿æŒ‡å®šç»´åº¦åšéå¯¹ç§°å‡åŒ€é‡åŒ–ã€‚

    å…¬å¼: q = round((x - zero_point) / scale), scale = (max - min) / (2^bits - 1)

    å‚æ•°:
        tensor: å¾…é‡åŒ– tensor
        bits: é‡åŒ–ä½å®½ (2 or 4)
        dim: é‡åŒ–ç»´åº¦ (æ²¿å“ªä¸ªç»´åº¦è®¡ç®— min/max)

    è¿”å›:
        tuple: (quantized_int, scale, zero_point)
    """
    qmin = 0
    qmax = (1 << bits) - 1  # 2-bit: 0~3, 4-bit: 0~15

    # æ²¿æŒ‡å®šç»´åº¦è®¡ç®— min/max
    t_min = tensor.amin(dim=dim, keepdim=True)
    t_max = tensor.amax(dim=dim, keepdim=True)

    # é˜²æ­¢ min == max (å¸¸é‡ tensor)
    t_range = (t_max - t_min).clamp(min=1e-8)

    scale = t_range / qmax
    zero_point = t_min

    # é‡åŒ–
    q = ((tensor - zero_point) / scale).round().clamp(qmin, qmax).to(torch.uint8)

    return q, scale, zero_point


def _asymmetric_dequantize(q: torch.Tensor, scale: torch.Tensor, zero_point: torch.Tensor,
                           dtype: torch.dtype) -> torch.Tensor:
    """
    åé‡åŒ–: x_hat = q * scale + zero_point
    """
    return q.to(dtype) * scale + zero_point


class KiviKVCache:
    """
    KIVI KV Cache ç®¡ç†å™¨ã€‚

    Key: per-channel é‡åŒ– (æ²¿ head_dim ç»´åº¦)
    Value: per-token é‡åŒ– (æ²¿ seq_len ç»´åº¦)
    æœ€è¿‘ residual_length ä¸ª token ä¿æŒ FP16ã€‚
    """

    def __init__(self, key_bits: int = 2, value_bits: int = 2,
                 group_size: int = 32, residual_length: int = 128):
        """
        å‚æ•°:
            key_bits: Key é‡åŒ–ä½å®½
            value_bits: Value é‡åŒ–ä½å®½
            group_size: é‡åŒ–åˆ†ç»„å¤§å°
            residual_length: ä¿æŒ FP16 çš„æœ€è¿‘ token æ•°
        """
        self.key_bits = key_bits
        self.value_bits = value_bits
        self.group_size = group_size
        self.residual_length = residual_length

        # å·²é‡åŒ–çš„ Key chunks: list of (q, scale, zero_point)
        self.quantized_key_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self.quantized_value_chunks: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []

        # æ®‹ä½™ (FP16)
        self.residual_key: torch.Tensor | None = None
        self.residual_value: torch.Tensor | None = None

        # ç»Ÿè®¡
        self.total_tokens_quantized = 0

    def update(self, key: torch.Tensor, value: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        æ¥æ”¶å®Œæ•´ KVï¼Œé‡åŒ–å†å²éƒ¨åˆ†ï¼Œä¿ç•™æœ€è¿‘ residual_length ä¸ª token ä¸º FP16ã€‚

        å‚æ•°:
            key: [B, H, S, D]
            value: [B, H, S, D]

        è¿”å›:
            tuple: (full_key, full_value) é‡æ„åçš„å®Œæ•´ KV
        """
        seq_len = key.size(2)
        dtype = key.dtype

        # æ¸…ç©ºæ—§çŠ¶æ€ (æ¯æ¬¡ä»å®Œæ•´ KV é‡æ–°é‡åŒ–)
        self.quantized_key_chunks.clear()
        self.quantized_value_chunks.clear()
        self.total_tokens_quantized = 0

        if seq_len <= self.residual_length:
            # åºåˆ—å¤ªçŸ­ï¼Œå…¨éƒ¨ä¿æŒ FP16
            self.residual_key = key
            self.residual_value = value
            return key, value

        # åˆ†ç¦»: å†å²éƒ¨åˆ†é‡åŒ–ï¼Œæœ€è¿‘éƒ¨åˆ†ä¿æŒ FP16
        split_point = seq_len - self.residual_length
        hist_key = key[:, :, :split_point, :]
        hist_value = value[:, :, :split_point, :]
        self.residual_key = key[:, :, split_point:, :].contiguous()
        self.residual_value = value[:, :, split_point:, :].contiguous()

        # é‡åŒ–å†å² Key: per-channel (æ²¿ D ç»´åº¦ï¼Œå³ dim=-1 çš„åˆ†ç»„)
        # KIVI çš„ per-channel æ„å‘³ç€æ¯ä¸ª channel æœ‰ç‹¬ç«‹çš„ scale/zp
        # å¯¹äº [B, H, T, D]ï¼Œper-channel = å¯¹æ¯ä¸ª d åˆ†åˆ«é‡åŒ– T ç»´
        # å®é™…æ“ä½œ: æ²¿ seq ç»´åº¦ (dim=2) è®¡ç®—ç»Ÿè®¡é‡ï¼Œè¿™æ ·æ¯ä¸ª channel æœ‰ç‹¬ç«‹å‚æ•°
        q_key, s_key, z_key = _asymmetric_quantize(hist_key, self.key_bits, dim=2)
        self.quantized_key_chunks.append((q_key, s_key, z_key))

        # é‡åŒ–å†å² Value: per-token (æ²¿ T ç»´åº¦)
        # per-token æ„å‘³ç€æ¯ä¸ª token æœ‰ç‹¬ç«‹çš„ scale/zp
        # å¯¹äº [B, H, T, D]ï¼Œper-token = æ²¿ head_dim ç»´åº¦ (dim=3) è®¡ç®—ç»Ÿè®¡é‡
        q_val, s_val, z_val = _asymmetric_quantize(hist_value, self.value_bits, dim=3)
        self.quantized_value_chunks.append((q_val, s_val, z_val))

        self.total_tokens_quantized = split_point

        # é‡æ„
        full_key = self._reconstruct_all(is_key=True, dtype=dtype)
        full_value = self._reconstruct_all(is_key=False, dtype=dtype)

        return full_key, full_value

    def _reconstruct_all(self, is_key: bool, dtype: torch.dtype) -> torch.Tensor:
        """é‡æ„å®Œæ•´ KV (é‡åŒ–éƒ¨åˆ†åé‡åŒ– + æ®‹ä½™ FP16)ã€‚"""
        chunks = self.quantized_key_chunks if is_key else self.quantized_value_chunks
        residual = self.residual_key if is_key else self.residual_value

        parts = []
        for q, s, z in chunks:
            parts.append(_asymmetric_dequantize(q, s, z, dtype))
        if residual is not None:
            parts.append(residual)

        if not parts:
            return torch.empty(0)
        return torch.cat(parts, dim=2)


def _patch_attention_layers_kivi(model: nn.Module, kivi_config: dict) -> list[KiviKVCache]:
    """
    Monkey-patch Attention å±‚ï¼Œæ¤å…¥ KIVI KV Cacheã€‚

    å…¼å®¹ LlamaAttention / Qwen2Attention / MistralAttentionã€‚
    """
    key_bits = kivi_config.get("key_bits", 2)
    value_bits = kivi_config.get("value_bits", 2)
    group_size = kivi_config.get("group_size", 32)
    residual_length = kivi_config.get("residual_length", 128)

    caches = []
    patched_count = 0

    for name, module in model.named_modules():
        module_type = type(module).__name__
        if "Attention" not in module_type:
            continue
        if not (hasattr(module, "k_proj") and hasattr(module, "v_proj")):
            continue

        cache = KiviKVCache(
            key_bits=key_bits,
            value_bits=value_bits,
            group_size=group_size,
            residual_length=residual_length,
        )
        caches.append(cache)

        original_forward = module.forward

        def make_patched_forward(orig_fwd, kivi_cache):
            def patched_forward(*args, **kwargs):
                outputs = orig_fwd(*args, **kwargs)

                if isinstance(outputs, tuple) and len(outputs) >= 3:
                    attn_output = outputs[0]
                    attn_weights = outputs[1]
                    past_kv = outputs[2]

                    if isinstance(past_kv, tuple) and len(past_kv) == 2:
                        key_states, value_states = past_kv
                        compressed_key, compressed_value = kivi_cache.update(
                            key_states, value_states
                        )
                        outputs = (attn_output, attn_weights, (compressed_key, compressed_value))

                return outputs
            return patched_forward

        module.forward = make_patched_forward(original_forward, cache)
        patched_count += 1
        print(f"  âš¡ KIVI Patched: {name} ({module_type})")

    print(f"ğŸ“‹ KIVI: å…± patch äº† {patched_count} ä¸ª Attention å±‚")
    return caches


@register("kivi")
class KiviMethod(BaseQuantMethod):
    """
    KIVI â€” Tuning-free Asymmetric 2-bit KV Cache Quantization.

    Key per-channel + Value per-token éå¯¹ç§°é‡åŒ–ã€‚
    """

    supported_tracks = ["C"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        kv_config = self.config.get("kv", {})
        key_bits = kv_config.get("key_bits", 2)
        value_bits = kv_config.get("value_bits", 2)
        group_size = kv_config.get("group_size", 32)
        residual_length = kv_config.get("residual_length", 128)

        print(f"ğŸ“‹ KIVI: key_bits={key_bits}, value_bits={value_bits}")
        print(f"ğŸ“‹ KIVI: group_size={group_size}, residual_length={residual_length}")
        print(f"ğŸ“‹ KIVI: å…æ ¡å‡†ï¼Œéå¯¹ç§° per-channel Key + per-token Value é‡åŒ–")

        kivi_config = {
            "key_bits": key_bits,
            "value_bits": value_bits,
            "group_size": group_size,
            "residual_length": residual_length,
        }

        caches = _patch_attention_layers_kivi(model, kivi_config)

        if not caches:
            print("âš ï¸  æœªæ‰¾åˆ°å¯ patch çš„ Attention å±‚")
        else:
            print(f"âœ… KIVI: {len(caches)} ä¸ª Attention å±‚å·²å¯ç”¨ INT{key_bits}/INT{value_bits} KV é‡åŒ–")

        model._kivi_caches = caches
        return model
