# -*- coding: utf-8 -*-
"""
KIVI â€” Tuning-free Asymmetric 2-bit KV Cache Quantization

æ ¸å¿ƒæ€è·¯:
- Key Cache: æŒ‰ **channel ç»´åº¦** åˆ†ç»„é‡åŒ– (per-channel)
  â†’ å› ä¸º Key çš„å¼‚å¸¸å€¼é›†ä¸­åœ¨å›ºå®š channel
- Value Cache: æŒ‰ **token ç»´åº¦** åˆ†ç»„é‡åŒ– (per-token)
  â†’ Value æ— æ˜æ˜¾ channel å¼‚å¸¸å€¼æ¨¡å¼
- æœªå‡‘æ»¡ group çš„æ®‹ä½™ token ä¿æŒ FP16
- å…æ ¡å‡†ï¼Œè¿è¡Œæ—¶åœ¨çº¿é‡åŒ–

ä¿®å¤è¯´æ˜ (2026-02-17):
  Transformers v5.x çš„ Attention ä¸å†é€šè¿‡ outputs[2] è¿”å› past_key_valuesï¼Œ
  è€Œæ˜¯é€šè¿‡ DynamicCache.layers[i].update() å°±åœ°ç®¡ç† KV Cacheã€‚
  å› æ­¤å°†é‡åŒ–é€»è¾‘ä» monkey-patch attention forward è¿ç§»åˆ°è‡ªå®šä¹‰ CacheLayerã€‚

å‚è€ƒ: Zirui Liu et al., "KIVI: A Tuning-Free Asymmetric 2bit Quantization
for KV Cache", ICML 2024.
"""

import torch
import torch.nn as nn
from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any
from transformers.cache_utils import DynamicLayer, DynamicCache


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é‡åŒ–/åé‡åŒ–å·¥å…·å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _asymmetric_quantize(tensor: torch.Tensor, bits: int, dim: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    æ²¿æŒ‡å®šç»´åº¦åšéå¯¹ç§°å‡åŒ€é‡åŒ–ã€‚

    å…¬å¼: q = round((x - zero_point) / scale), scale = (max - min) / (2^bits - 1)
    """
    qmin = 0
    qmax = (1 << bits) - 1

    t_min = tensor.amin(dim=dim, keepdim=True)
    t_max = tensor.amax(dim=dim, keepdim=True)
    t_range = (t_max - t_min).clamp(min=1e-8)

    scale = t_range / qmax
    zero_point = t_min

    q = ((tensor - zero_point) / scale).round().clamp(qmin, qmax).to(torch.uint8)
    return q, scale, zero_point


def _asymmetric_dequantize(q: torch.Tensor, scale: torch.Tensor, zero_point: torch.Tensor,
                           dtype: torch.dtype) -> torch.Tensor:
    """åé‡åŒ–: x_hat = q * scale + zero_point"""
    return q.to(dtype) * scale + zero_point


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KiviCacheLayer â€” å­ç±»åŒ– DynamicLayer
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KiviCacheLayer(DynamicLayer):
    """
    KIVI é‡åŒ–çš„ Cache Layerã€‚

    ç»§æ‰¿ DynamicLayerï¼Œè¦†å†™ update() æ¥æ‹¦æˆª KV states å¹¶é‡åŒ–ã€‚
    Key: per-channel é‡åŒ– (æ²¿ head_dim ç»´åº¦åˆ†ç»„)
    Value: per-token é‡åŒ– (æ²¿ seq_len ç»´åº¦åˆ†ç»„)
    æœ€è¿‘ residual_length ä¸ª token ä¿æŒ FP16ã€‚
    """

    def __init__(self, key_bits: int = 2, value_bits: int = 2,
                 residual_length: int = 128, layer_idx: int = 0):
        super().__init__()
        self.key_bits = key_bits
        self.value_bits = value_bits
        self.residual_length = residual_length
        self.layer_idx = layer_idx
        self.cumulative_length = 0

        # é‡åŒ–å­˜å‚¨
        self._quantized_key: tuple | None = None  # (q, scale, zero_point)
        self._quantized_value: tuple | None = None
        self._residual_key: torch.Tensor | None = None
        self._residual_value: torch.Tensor | None = None

        # debug ç»Ÿè®¡
        self._quantize_count = 0

    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        cache_kwargs: dict[str, Any] | None = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        æ‹¦æˆª KV æ›´æ–°ï¼Œé‡åŒ–å†å²éƒ¨åˆ†ï¼Œä¿ç•™æœ€è¿‘ residual_length ä¸º FP16ã€‚

        æµç¨‹:
        1. ç”¨ torch.cat ç´¯ç§¯æ–° KV åˆ°å®Œæ•´åºåˆ—
        2. å¦‚æœæ€»é•¿åº¦ > residual_length â†’ åˆ†ç¦»ä¸ºå†å²+æ®‹å·®
        3. å†å²éƒ¨åˆ†é‡åŒ– (Key per-channel, Value per-token)
        4. è¿”å› dequant(å†å²) + æ®‹å·®
        """
        if not self.is_initialized:
            self.lazy_initialization(key_states, value_states)

        self.cumulative_length += key_states.shape[-2]
        dtype = key_states.dtype

        # Step 1: ç´¯ç§¯å®Œæ•´ KV
        if self._quantized_key is not None:
            # å·²æœ‰é‡åŒ–å†å² â†’ æ‹¼æ¥: dequant(å†å²) + æ®‹å·® + æ–°token
            dequant_key = _asymmetric_dequantize(*self._quantized_key, dtype)
            dequant_value = _asymmetric_dequantize(*self._quantized_value, dtype)
            parts_key = [dequant_key]
            parts_value = [dequant_value]
            if self._residual_key is not None and self._residual_key.numel() > 0:
                parts_key.append(self._residual_key)
                parts_value.append(self._residual_value)
            parts_key.append(key_states)
            parts_value.append(value_states)
            full_key = torch.cat(parts_key, dim=-2)
            full_value = torch.cat(parts_value, dim=-2)
        else:
            # é¦–æ¬¡æˆ–ä»…æœ‰æ®‹å·®
            if self.keys.numel() > 0:
                full_key = torch.cat([self.keys, key_states], dim=-2)
                full_value = torch.cat([self.values, value_states], dim=-2)
            else:
                full_key = key_states
                full_value = value_states

        seq_len = full_key.shape[-2]

        # Step 2: å†³å®šæ˜¯å¦é‡åŒ–
        if seq_len <= self.residual_length:
            # åºåˆ—å¤ªçŸ­ï¼Œå…¨éƒ¨ä¿æŒ FP16
            self.keys = full_key
            self.values = full_value
            self._quantized_key = None
            self._quantized_value = None
            self._residual_key = None
            self._residual_value = None
            return full_key, full_value

        # Step 3: åˆ†ç¦»å†å² + æ®‹å·®
        split_point = seq_len - self.residual_length
        hist_key = full_key[:, :, :split_point, :].contiguous()
        hist_value = full_value[:, :, :split_point, :].contiguous()
        self._residual_key = full_key[:, :, split_point:, :].contiguous()
        self._residual_value = full_value[:, :, split_point:, :].contiguous()

        # Step 4: é‡åŒ–å†å²éƒ¨åˆ†
        # Key: per-channel (æ²¿ seq ç»´åº¦ dim=2 è®¡ç®—ç»Ÿè®¡é‡ï¼Œæ¯ä¸ª channel ç‹¬ç«‹)
        self._quantized_key = _asymmetric_quantize(hist_key, self.key_bits, dim=2)
        # Value: per-token (æ²¿ head_dim ç»´åº¦ dim=3 è®¡ç®—ç»Ÿè®¡é‡ï¼Œæ¯ä¸ª token ç‹¬ç«‹)
        self._quantized_value = _asymmetric_quantize(hist_value, self.value_bits, dim=3)

        self._quantize_count += 1

        # æ¸…ç©º self.keys/values (å†å²å·²é‡åŒ–)
        self.keys = torch.tensor([], dtype=dtype, device=key_states.device)
        self.values = torch.tensor([], dtype=dtype, device=key_states.device)

        # Step 5: é‡æ„è¿”å›
        dequant_key = _asymmetric_dequantize(*self._quantized_key, dtype)
        dequant_value = _asymmetric_dequantize(*self._quantized_value, dtype)
        return_key = torch.cat([dequant_key, self._residual_key], dim=-2)
        return_value = torch.cat([dequant_value, self._residual_value], dim=-2)

        return return_key, return_value

    def get_seq_length(self) -> int:
        return self.cumulative_length


class KiviQuantizedCache(DynamicCache):
    """
    ç”¨ KiviCacheLayer æ›¿ä»£ DynamicLayer çš„ DynamicCacheã€‚
    """

    def __init__(self, key_bits: int = 2, value_bits: int = 2,
                 residual_length: int = 128, num_layers: int = 32,
                 **kwargs):
        # æ„é€  KiviCacheLayer åˆ—è¡¨
        layers = [
            KiviCacheLayer(
                key_bits=key_bits,
                value_bits=value_bits,
                residual_length=residual_length,
                layer_idx=i,
            )
            for i in range(num_layers)
        ]
        # ç”¨ Cache åŸºç±»åˆå§‹åŒ– (è·³è¿‡ DynamicCache.__init__ çš„ config é€»è¾‘)
        from transformers.cache_utils import Cache
        Cache.__init__(self, layers=layers)

    def get_quantize_stats(self) -> dict:
        """è¿”å›é‡åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        stats = {}
        for i, layer in enumerate(self.layers):
            if isinstance(layer, KiviCacheLayer) and layer._quantize_count > 0:
                stats[i] = {
                    "quantize_count": layer._quantize_count,
                    "cumulative_length": layer.cumulative_length,
                }
        return stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ³¨å…¥æœºåˆ¶: hook model.generate ä¼ å…¥è‡ªå®šä¹‰ cache
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _inject_kivi_cache(model: nn.Module, kivi_config: dict):
    """
    Monkey-patch model.generate() ä½¿å…¶ä½¿ç”¨ KiviQuantizedCacheã€‚

    åœ¨ generate è°ƒç”¨æ—¶ï¼Œå¦‚æœæ²¡æœ‰æ˜¾å¼ä¼ å…¥ past_key_valuesï¼Œ
    åˆ™è‡ªåŠ¨åˆ›å»º KiviQuantizedCache æ³¨å…¥ã€‚
    """
    key_bits = kivi_config["key_bits"]
    value_bits = kivi_config["value_bits"]
    residual_length = kivi_config["residual_length"]

    # è·å–å±‚æ•°
    num_layers = model.config.num_hidden_layers

    original_generate = model.generate

    def patched_generate(*args, **kwargs):
        # å¦‚æœæ²¡æœ‰æ˜¾å¼ä¼ å…¥ cacheï¼Œæ³¨å…¥ KiviQuantizedCache
        if "past_key_values" not in kwargs or kwargs["past_key_values"] is None:
            cache = KiviQuantizedCache(
                key_bits=key_bits,
                value_bits=value_bits,
                residual_length=residual_length,
                num_layers=num_layers,
            )
            kwargs["past_key_values"] = cache

        result = original_generate(*args, **kwargs)

        # æ‰“å°é‡åŒ–ç»Ÿè®¡ (é¦–æ¬¡è°ƒç”¨æ—¶)
        if hasattr(model, "_kivi_stats_printed"):
            return result
        model._kivi_stats_printed = True

        if "past_key_values" in kwargs and isinstance(kwargs["past_key_values"], KiviQuantizedCache):
            stats = kwargs["past_key_values"].get_quantize_stats()
            if stats:
                sample_layer = next(iter(stats.values()))
                print(f"  ğŸ“Š KIVI é‡åŒ–ç¡®è®¤: layer 0 é‡åŒ–äº† {sample_layer['quantize_count']} æ¬¡, "
                      f"ç´¯è®¡ {sample_layer['cumulative_length']} tokens")
            else:
                print("  âš ï¸ KIVI: é‡åŒ–æœªè§¦å‘ (åºåˆ—å¯èƒ½å¤ªçŸ­)")

        return result

    model.generate = patched_generate

    # åŒæ—¶ hook model.forward / model.__call__ ä»¥æ”¯æŒ PPL è¯„æµ‹ (é generate åœºæ™¯)
    # PPL è¯„æµ‹ç›´æ¥è°ƒç”¨ model(input_ids)ï¼Œä¸ç»è¿‡ generate
    # ä½† PPL è¯„æµ‹æ¯ä¸ªçª—å£ç‹¬ç«‹ forwardï¼ŒKV Cache ä¸è·¨çª—å£ï¼Œæ‰€ä»¥é‡åŒ–ä¹Ÿä¸éœ€è¦
    # è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç¡®ä¿ generate åœºæ™¯ä¸‹é‡åŒ–ç”Ÿæ•ˆ

    print(f"  âœ… KIVI Cache æ³¨å…¥å®Œæˆ: {num_layers} å±‚, "
          f"INT{key_bits}/INT{value_bits}, residual={residual_length}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KiviMethod (æ³¨å†Œåˆ° registry)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@register("kivi")
class KiviMethod(BaseQuantMethod):
    """
    KIVI â€” Tuning-free Asymmetric 2-bit KV Cache Quantization.

    Key per-channel + Value per-token éå¯¹ç§°é‡åŒ–ã€‚
    é€šè¿‡è‡ªå®šä¹‰ DynamicCache å®ç°ï¼Œå…¼å®¹ Transformers v5.xã€‚
    """

    supported_tracks = ["C"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        kv_config = self.config.get("kv", {})
        key_bits = kv_config.get("key_bits", 2)
        value_bits = kv_config.get("value_bits", 2)
        group_size = kv_config.get("group_size", 32)
        residual_length = kv_config.get("residual_length", 128)

        print(f"ğŸ“‹ KIVI: key_bits={key_bits}, value_bits={value_bits}")
        print(f"ğŸ“‹ KIVI: group_size={group_size}, residual_length={residual_length}")
        print(f"ğŸ“‹ KIVI: å…æ ¡å‡†ï¼Œéå¯¹ç§° per-channel Key + per-token Value é‡åŒ–")

        kivi_config = {
            "key_bits": key_bits,
            "value_bits": value_bits,
            "group_size": group_size,
            "residual_length": residual_length,
        }

        _inject_kivi_cache(model, kivi_config)

        return model
