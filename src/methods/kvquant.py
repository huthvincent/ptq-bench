# -*- coding: utf-8 -*-
"""
KVQuant â€” Sensitivity-Weighted KV Cache Quantization with Outlier Isolation

æ ¸å¿ƒæ€è·¯:
- Key Cache: per-channel é‡åŒ– (æ²¿ channel ç»´åº¦è®¡ç®—ç»Ÿè®¡é‡)
- Value Cache: per-token é‡åŒ– (æ²¿ token ç»´åº¦è®¡ç®—ç»Ÿè®¡é‡)
- Dense-and-Sparse: æ¯ä¸ªå‘é‡éš”ç¦» top-k å¼‚å¸¸å€¼ï¼Œå•ç‹¬ FP16 å­˜å‚¨
- åé‡åŒ–æ—¶: ä½ç²¾åº¦ dense éƒ¨åˆ† + FP16 sparse å¼‚å¸¸å€¼
- éœ€è¦æ ¡å‡†æ•°æ®è®¡ç®—æœ€ä¼˜ scale (æœ¬å®ç°ç”¨ min/max å‡åŒ€é‡åŒ–ç®€åŒ–)

å‚è€ƒ: Coleman Hooper et al., "KVQuant: Towards 10 Million Context Length
LLM Inference with KV Cache Quantization", 2024.
"""

import torch
import torch.nn as nn
from src.registry import register
from src.methods.base import BaseQuantMethod
from typing import Any


def _quantize_with_outliers(tensor: torch.Tensor, bits: int, quant_dim: int,
                            num_outliers: int = 1) -> dict:
    """
    Dense-and-Sparse é‡åŒ–: éš”ç¦»å¼‚å¸¸å€¼åé‡åŒ–ã€‚

    æ­¥éª¤:
    1. æ²¿æ¯ä¸ªå‘é‡æ‰¾ top-k ç»å¯¹å€¼æœ€å¤§çš„å…ƒç´  (sparse outliers)
    2. å°† outlier ä½ç½®ç½®é›¶
    3. å¯¹å‰©ä½™éƒ¨åˆ†åšå‡åŒ€é‡åŒ– (dense)
    4. åˆ†åˆ«å­˜å‚¨ dense (INT) å’Œ sparse (FP16)

    å‚æ•°:
        tensor: [B, H, T, D]
        bits: é‡åŒ–ä½å®½
        quant_dim: é‡åŒ–ç»´åº¦ (Key: dim=2 per-channel, Value: dim=3 per-token)
        num_outliers: æ¯ä¸ªå‘é‡éš”ç¦»çš„å¼‚å¸¸å€¼ä¸ªæ•°

    è¿”å›:
        dict: {q, scale, zero_point, outlier_values, outlier_indices}
    """
    dtype = tensor.dtype
    B, H, T, D = tensor.shape
    qmin = 0
    qmax = (1 << bits) - 1

    if num_outliers > 0:
        # æ‰¾å¼‚å¸¸å€¼: æ²¿ "éé‡åŒ–" ç»´åº¦çš„æ¯ä¸ªå‘é‡ä¸­æ‰¾ top-k
        # å¯¹äº per-channel Key (quant_dim=2): æ¯ä¸ª [B,H,:,d] å‘é‡æ‰¾ outlier â†’ å¤ªå¤æ‚
        # ç®€åŒ–: æ²¿æœ€åä¸€ä¸ªç»´åº¦ (D) æ‰¾æ¯ä¸ª token çš„ outlier
        abs_tensor = tensor.abs()
        # topk æ²¿ D ç»´åº¦
        _, outlier_idx = abs_tensor.topk(num_outliers, dim=-1)  # [B, H, T, k]

        # æå– outlier å€¼
        outlier_vals = torch.gather(tensor, dim=-1, index=outlier_idx)  # [B, H, T, k]

        # åˆ›å»º mask å¹¶ç½®é›¶ outlier
        dense_tensor = tensor.clone()
        dense_tensor.scatter_(dim=-1, index=outlier_idx, value=0.0)
    else:
        dense_tensor = tensor
        outlier_vals = None
        outlier_idx = None

    # å‡åŒ€é‡åŒ– dense éƒ¨åˆ†
    t_min = dense_tensor.amin(dim=quant_dim, keepdim=True)
    t_max = dense_tensor.amax(dim=quant_dim, keepdim=True)
    t_range = (t_max - t_min).clamp(min=1e-8)

    scale = t_range / qmax
    zero_point = t_min

    q = ((dense_tensor - zero_point) / scale).round().clamp(qmin, qmax).to(torch.uint8)

    return {
        "q": q,
        "scale": scale,
        "zero_point": zero_point,
        "outlier_values": outlier_vals,
        "outlier_indices": outlier_idx,
    }


def _dequantize_with_outliers(qdata: dict, dtype: torch.dtype) -> torch.Tensor:
    """
    åé‡åŒ–: dense éƒ¨åˆ† + sparse outlier åŠ å›ã€‚
    """
    q = qdata["q"]
    scale = qdata["scale"]
    zero_point = qdata["zero_point"]

    # dense åé‡åŒ–
    result = q.to(dtype) * scale + zero_point

    # åŠ å› outliers
    if qdata["outlier_values"] is not None:
        result.scatter_(dim=-1, index=qdata["outlier_indices"], src=qdata["outlier_values"])

    return result


class KVQuantCache:
    """
    KVQuant KV Cache ç®¡ç†å™¨ã€‚

    Key: per-channel é‡åŒ– + outlier éš”ç¦»
    Value: per-token é‡åŒ– + outlier éš”ç¦»
    æœ€è¿‘ residual_length ä¸ª token ä¿æŒ FP16ã€‚
    """

    def __init__(self, key_bits: int = 2, value_bits: int = 2,
                 num_outliers: int = 1, residual_length: int = 128):
        self.key_bits = key_bits
        self.value_bits = value_bits
        self.num_outliers = num_outliers
        self.residual_length = residual_length

        self.quantized_key: dict | None = None
        self.quantized_value: dict | None = None
        self.residual_key: torch.Tensor | None = None
        self.residual_value: torch.Tensor | None = None
        self.total_tokens_quantized = 0

    def update(self, key: torch.Tensor, value: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        æ¥æ”¶å®Œæ•´ KVï¼Œé‡åŒ–å†å²éƒ¨åˆ†ï¼Œä¿ç•™æœ€è¿‘ residual_length ä¸ª token ä¸º FP16ã€‚
        """
        seq_len = key.size(2)
        dtype = key.dtype

        self.quantized_key = None
        self.quantized_value = None
        self.total_tokens_quantized = 0

        if seq_len <= self.residual_length:
            self.residual_key = key
            self.residual_value = value
            return key, value

        split_point = seq_len - self.residual_length
        hist_key = key[:, :, :split_point, :]
        hist_value = value[:, :, :split_point, :]
        self.residual_key = key[:, :, split_point:, :].contiguous()
        self.residual_value = value[:, :, split_point:, :].contiguous()

        # Key: per-channel (quant_dim=2, æ²¿ seq ç»´åº¦è®¡ç®—ç»Ÿè®¡é‡ â†’ æ¯ä¸ª channel ç‹¬ç«‹)
        self.quantized_key = _quantize_with_outliers(
            hist_key, self.key_bits, quant_dim=2, num_outliers=self.num_outliers
        )

        # Value: per-token (quant_dim=3, æ²¿ head_dim ç»´åº¦è®¡ç®—ç»Ÿè®¡é‡ â†’ æ¯ä¸ª token ç‹¬ç«‹)
        self.quantized_value = _quantize_with_outliers(
            hist_value, self.value_bits, quant_dim=3, num_outliers=self.num_outliers
        )

        self.total_tokens_quantized = split_point

        # é‡æ„
        full_key = self._reconstruct(is_key=True, dtype=dtype)
        full_value = self._reconstruct(is_key=False, dtype=dtype)

        return full_key, full_value

    def _reconstruct(self, is_key: bool, dtype: torch.dtype) -> torch.Tensor:
        qdata = self.quantized_key if is_key else self.quantized_value
        residual = self.residual_key if is_key else self.residual_value

        parts = []
        if qdata is not None:
            parts.append(_dequantize_with_outliers(qdata, dtype))
        if residual is not None:
            parts.append(residual)

        if not parts:
            return torch.empty(0)
        return torch.cat(parts, dim=2)


def _patch_attention_layers_kvquant(model: nn.Module, kvq_config: dict) -> list[KVQuantCache]:
    """
    Monkey-patch Attention å±‚ï¼Œæ¤å…¥ KVQuant Cacheã€‚
    """
    key_bits = kvq_config.get("key_bits", 2)
    value_bits = kvq_config.get("value_bits", 2)
    num_outliers = kvq_config.get("num_outliers", 1)
    residual_length = kvq_config.get("residual_length", 128)

    caches = []
    patched_count = 0

    for name, module in model.named_modules():
        module_type = type(module).__name__
        if "Attention" not in module_type:
            continue
        if not (hasattr(module, "k_proj") and hasattr(module, "v_proj")):
            continue

        cache = KVQuantCache(
            key_bits=key_bits,
            value_bits=value_bits,
            num_outliers=num_outliers,
            residual_length=residual_length,
        )
        caches.append(cache)

        original_forward = module.forward

        def make_patched_forward(orig_fwd, kvq_cache):
            def patched_forward(*args, **kwargs):
                outputs = orig_fwd(*args, **kwargs)

                if isinstance(outputs, tuple) and len(outputs) >= 3:
                    attn_output = outputs[0]
                    attn_weights = outputs[1]
                    past_kv = outputs[2]

                    if isinstance(past_kv, tuple) and len(past_kv) == 2:
                        key_states, value_states = past_kv
                        compressed_key, compressed_value = kvq_cache.update(
                            key_states, value_states
                        )
                        outputs = (attn_output, attn_weights, (compressed_key, compressed_value))

                return outputs
            return patched_forward

        module.forward = make_patched_forward(original_forward, cache)
        patched_count += 1
        print(f"  âš¡ KVQuant Patched: {name} ({module_type})")

    print(f"ğŸ“‹ KVQuant: å…± patch äº† {patched_count} ä¸ª Attention å±‚")
    return caches


@register("kvquant")
class KVQuantMethod(BaseQuantMethod):
    """
    KVQuant â€” Sensitivity-Weighted KV Cache Quantization with Outlier Isolation.
    """

    supported_tracks = ["C"]

    def quantize(self, model: Any, tokenizer: Any, calib_data: Any | None = None) -> Any:
        kv_config = self.config.get("kv", {})
        key_bits = kv_config.get("key_bits", 2)
        value_bits = kv_config.get("value_bits", 2)
        num_outliers = kv_config.get("num_outliers", 1)
        residual_length = kv_config.get("residual_length", 128)

        print(f"ğŸ“‹ KVQuant: key_bits={key_bits}, value_bits={value_bits}")
        print(f"ğŸ“‹ KVQuant: num_outliers={num_outliers}, residual_length={residual_length}")
        print(f"ğŸ“‹ KVQuant: per-channel Key + per-token Value + Dense-and-Sparse outlier éš”ç¦»")

        kvq_config = {
            "key_bits": key_bits,
            "value_bits": value_bits,
            "num_outliers": num_outliers,
            "residual_length": residual_length,
        }

        caches = _patch_attention_layers_kvquant(model, kvq_config)

        if not caches:
            print("âš ï¸  æœªæ‰¾åˆ°å¯ patch çš„ Attention å±‚")
        else:
            print(f"âœ… KVQuant: {len(caches)} ä¸ª Attention å±‚å·²å¯ç”¨ INT{key_bits}/INT{value_bits} + outlier éš”ç¦»")

        model._kvquant_caches = caches
        return model
