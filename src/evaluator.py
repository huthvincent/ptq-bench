# -*- coding: utf-8 -*-
"""
è¯„æµ‹å¼•æ“

è´Ÿè´£å¯¹é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼ŒåŒ…æ‹¬ï¼š
- PPL (Perplexity) è¯„æµ‹ï¼šåœ¨ WikiText-2 / C4 ä¸Šè®¡ç®—å›°æƒ‘åº¦
- lm-eval-harness ä»»åŠ¡è¯„æµ‹ï¼šMMLUã€GSM8Kã€HellaSwag ç­‰
- ç³»ç»ŸæŒ‡æ ‡è¯„æµ‹ï¼ˆPhase 2ï¼‰ï¼šTTFTã€ååé‡ã€VRAM å³°å€¼

ä¸»è¦å…¥å£å‡½æ•°ï¼š
- evaluate(): æ ¹æ®é…ç½®è¿è¡Œå…¨éƒ¨è¯„æµ‹
- evaluate_ppl(): åªè·‘ PPL
- evaluate_lm_eval(): åªè·‘ lm-eval ä»»åŠ¡
"""

import time
import json
from typing import Any
from pathlib import Path


def evaluate(model: Any, tokenizer: Any, config: dict) -> dict:
    """
    æ ¹æ®é…ç½®è¿è¡Œå…¨éƒ¨è¯„æµ‹ã€‚

    è¿™æ˜¯è¯„æµ‹çš„ä¸»å…¥å£å‡½æ•°ï¼Œä¼šæ ¹æ®é…ç½®ä¸­çš„ eval è®¾ç½®
    ä¾æ¬¡è¿è¡Œ PPL è¯„æµ‹å’Œ lm-eval ä»»åŠ¡è¯„æµ‹ã€‚

    å‚æ•°:
        model: é‡åŒ–åï¼ˆæˆ– FP16 baselineï¼‰çš„æ¨¡å‹
        tokenizer: å¯¹åº”çš„ tokenizer
        config: åˆå¹¶åçš„å®Œæ•´é…ç½®å­—å…¸

    è¿”å›:
        dict: è¯„æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å« pplã€lm_eval_resultsã€system_metrics ç­‰
    """
    results = {
        "ppl": {},
        "lm_eval": {},
        "system_metrics": {},
        "eval_time_seconds": 0,
    }

    start_time = time.time()

    # --- PPL è¯„æµ‹ ---
    eval_config = config.get("eval", config.get("default_eval", {}))
    # å…¼å®¹ä¸¤ç§åµŒå¥—ç»“æ„:
    # 1. default_eval é£æ ¼: eval.core_quality.ppl_datasets
    # 2. track é£æ ¼: eval.ppl_datasets (å¹³é“º)
    core_quality = eval_config.get("core_quality", {})
    ppl_datasets = core_quality.get("ppl_datasets", eval_config.get("ppl_datasets", ["wikitext2"]))

    for dataset_name in ppl_datasets:
        print(f"\nğŸ“Š è¯„æµ‹ PPL: {dataset_name}")
        ppl = evaluate_ppl(model, tokenizer, dataset_name, config)
        results["ppl"][dataset_name] = ppl

    # --- lm-eval ä»»åŠ¡è¯„æµ‹ ---
    lm_eval_tasks = core_quality.get("lm_eval_tasks", eval_config.get("lm_eval_tasks", []))
    if lm_eval_tasks:
        print(f"\nğŸ“Š è¯„æµ‹ lm-eval ä»»åŠ¡: {', '.join(lm_eval_tasks)}")
        lm_eval_results = evaluate_lm_eval(model, tokenizer, lm_eval_tasks, config)
        results["lm_eval"] = lm_eval_results

    # --- KV Cache å‹åŠ›æµ‹è¯• ---
    kv_stress = eval_config.get("kv_stress_test", {})
    if kv_stress.get("enabled", False):
        results["kv_stress_test"] = {}

        # Passkey Retrieval
        pk_config = kv_stress.get("passkey_retrieval", {})
        if pk_config.get("enabled", True):
            print("\nğŸ”‘ è¯„æµ‹ Passkey Retrieval")
            pk_result = evaluate_passkey_retrieval(model, tokenizer, pk_config)
            results["kv_stress_test"]["passkey_retrieval"] = pk_result

        # Generation PPL
        gp_config = kv_stress.get("generation_ppl", {})
        if gp_config.get("enabled", True):
            print("\nğŸ“ è¯„æµ‹ Generation PPL")
            gp_result = evaluate_generation_ppl(model, tokenizer, gp_config)
            results["kv_stress_test"]["generation_ppl"] = gp_result

    # --- ç³»ç»ŸæŒ‡æ ‡ï¼ˆPhase 2ï¼‰---
    system_config = eval_config.get("system_metrics", {})
    if isinstance(system_config, dict) and system_config.get("enabled", False):
        print("\nğŸ“Š ç³»ç»ŸæŒ‡æ ‡è¯„æµ‹ï¼ˆPhase 2 åŠŸèƒ½ï¼‰")
        results["system_metrics"] = {"status": "phase2_not_implemented"}

    # --- VRAM å³°å€¼ï¼ˆå¦‚æœ torch å¯ç”¨ï¼‰---
    try:
        import torch
        if torch.cuda.is_available():
            vram_peak_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)
            results["system_metrics"]["vram_peak_mb"] = round(vram_peak_mb, 1)
            print(f"ğŸ“Š VRAM å³°å€¼: {vram_peak_mb:.1f} MB")
    except ImportError:
        pass

    results["eval_time_seconds"] = round(time.time() - start_time, 1)
    return results


def evaluate_ppl(model: Any, tokenizer: Any, dataset_name: str, config: dict) -> dict:
    """
    åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè®¡ç®—æ¨¡å‹çš„ PPL (Perplexity)ã€‚

    ä½¿ç”¨æ ‡å‡†çš„ sliding window æ–¹æ³•è®¡ç®— PPLï¼š
    å°†æµ‹è¯•æ–‡æœ¬ tokenize åï¼ŒæŒ‰ max_seq_len æ»‘åŠ¨çª—å£è®¡ç®— NLLï¼Œ
    æœ€ç»ˆå– exp(avg_nll) ä½œä¸º PPLã€‚

    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: tokenizer
        dataset_name: æ•°æ®é›†åç§°ï¼ˆ"wikitext2" æˆ– "c4"ï¼‰
        config: é…ç½®å­—å…¸

    è¿”å›:
        dict: {"ppl": float, "nll": float, "num_tokens": int}
    """
    max_seq_len = config.get("common_hyperparams", {}).get("max_seq_len", 2048)

    try:
        import torch
        from datasets import load_dataset

        # åŠ è½½æ•°æ®é›†
        if dataset_name == "wikitext2":
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
            text = "\n\n".join(dataset["text"])
        elif dataset_name == "c4":
            dataset = load_dataset("allenai/c4", "en", split="validation", streaming=True)
            # C4 å¾ˆå¤§ï¼Œåªå–å‰ 256 æ¡
            texts = []
            for i, item in enumerate(dataset):
                if i >= 256:
                    break
                texts.append(item["text"])
            text = "\n\n".join(texts)
        elif dataset_name == "longbench":
            # é•¿ä¸Šä¸‹æ–‡ PPLï¼šä½¿ç”¨ pg19 é•¿ä¹¦ç±æ–‡æœ¬
            # pg19-test: 100 æ¡é•¿ä¹¦ç±ï¼Œæ¯æ¡ ~250K chars
            dataset = load_dataset("emozilla/pg19-test", split="test")
            # å–å‰ 10 æ¡é•¿æ–‡æ¡£æ‹¼æ¥ï¼ˆè¶³å¤Ÿäº§ç”Ÿå¤§é‡ tokenï¼‰
            texts = []
            for i, item in enumerate(dataset):
                if i >= 10:
                    break
                texts.append(item["text"])
            text = "\n\n".join(texts)
            # ä½¿ç”¨æ›´å¤§çš„ max_seq_len
            long_ctx = config.get("eval", {}).get("long_context", {})
            if long_ctx.get("enabled", False):
                max_seq_len = long_ctx.get("max_seq_len", 32768)
                print(f"  ğŸ“ é•¿ä¸Šä¸‹æ–‡: ä½¿ç”¨ max_seq_len={max_seq_len}")
        else:
            print(f"âš ï¸  æœªçŸ¥æ•°æ®é›† {dataset_name}ï¼Œè·³è¿‡ PPL è¯„æµ‹")
            return {"ppl": None, "error": f"unknown dataset: {dataset_name}"}

        # Tokenize
        encodings = tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids.to(model.device)
        seq_len = input_ids.size(1)

        print(f"  æ•°æ®é›† token æ•°: {seq_len}")

        # Sliding window PPL è®¡ç®—
        nlls = []
        stride = max_seq_len // 2  # ä½¿ç”¨ 50% é‡å 
        for begin in range(0, seq_len - max_seq_len, stride):
            end = begin + max_seq_len
            input_chunk = input_ids[:, begin:end]
            target_chunk = input_chunk.clone()

            # åªè®¡ç®—éé‡å éƒ¨åˆ†çš„ lossï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªçª—å£ï¼‰
            if begin > 0:
                target_chunk[:, :stride] = -100

            with torch.no_grad():
                outputs = model(input_chunk, labels=target_chunk)
                nll = outputs.loss.item()
                nlls.append(nll)

        import math
        avg_nll = sum(nlls) / len(nlls) if nlls else float("inf")
        ppl = math.exp(avg_nll)

        print(f"  PPL: {ppl:.2f}")
        return {"ppl": round(ppl, 4), "nll": round(avg_nll, 6), "num_windows": len(nlls)}

    except ImportError as e:
        print(f"âš ï¸  PPL è¯„æµ‹ä¾èµ–ç¼ºå¤±: {e}")
        return {"ppl": None, "error": str(e)}
    except Exception as e:
        print(f"âŒ PPL è¯„æµ‹å‡ºé”™: {e}")
        return {"ppl": None, "error": str(e)}


def evaluate_passkey_retrieval(model: Any, tokenizer: Any, config: dict) -> dict:
    """
    Passkey Retrieval è¯„æµ‹ â€” KV Cache å‹åŠ›æµ‹è¯•ã€‚

    åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­éšæœºä½ç½®æ’å…¥ä¸€ä¸ª 5 ä½æ•°å­—å¯†é’¥ï¼Œ
    ç”¨ model.generate() ç”Ÿæˆå›ç­”ï¼Œæ£€æŸ¥æ˜¯å¦èƒ½ç²¾ç¡®è¿˜åŸã€‚

    KV Cache ä¼šåœ¨ prefill é˜¶æ®µå¢é•¿åˆ° context_lengthï¼Œ
    ç„¶ååœ¨ decode é˜¶æ®µç»§ç»­å¢é•¿ â€” é‡åŒ–è¯¯å·®æŒç»­ç§¯ç´¯ã€‚
    """
    import torch
    import random

    num_keys = config.get("num_keys", 20)
    context_length = config.get("context_length", 2048)
    depths = config.get("depths", [0.1, 0.25, 0.5, 0.75])
    seed = config.get("seed", 42)

    random.seed(seed)
    torch.manual_seed(seed)

    # å¡«å……å¥ â€” ç”¨é‡å¤çš„æ— æ„ä¹‰å¥å­å¡«å……ä¸Šä¸‹æ–‡
    filler = "The grass is green. The sky is blue. The sun is yellow. Today is a beautiful day. "

    results_by_depth = {str(d): {"correct": 0, "total": 0} for d in depths}
    all_results = []

    for trial in range(num_keys):
        passkey = str(random.randint(10000, 99999))

        for depth in depths:
            # æ„å»ºä¸Šä¸‹æ–‡: filler + passkey + filler + question
            question = f"\nWhat is the passkey? The passkey is "
            passkey_sentence = f"\nThe passkey to remember is {passkey}. Remember it.\n"

            # ä¼°ç®— filler token æ•°ä»¥è¾¾åˆ°ç›®æ ‡ context_length
            q_tokens = len(tokenizer.encode(question, add_special_tokens=False))
            pk_tokens = len(tokenizer.encode(passkey_sentence, add_special_tokens=False))
            filler_unit_tokens = len(tokenizer.encode(filler, add_special_tokens=False))

            target_filler_tokens = context_length - q_tokens - pk_tokens
            if target_filler_tokens <= 0:
                target_filler_tokens = 512

            # åœ¨ depth ä½ç½®æ’å…¥ passkey
            filler_before_tokens = int(target_filler_tokens * depth)
            filler_after_tokens = target_filler_tokens - filler_before_tokens

            repeats_before = max(1, filler_before_tokens // filler_unit_tokens)
            repeats_after = max(1, filler_after_tokens // filler_unit_tokens)

            text = (filler * repeats_before) + passkey_sentence + (filler * repeats_after) + question

            # Tokenize
            inputs = tokenizer(text, return_tensors="pt", truncation=True,
                               max_length=context_length)
            input_ids = inputs["input_ids"].to(model.device)
            actual_len = input_ids.size(1)

            # Generate â€” KV Cache åœ¨è¿™é‡ŒæŒç»­å¢é•¿
            with torch.no_grad():
                output_ids = model.generate(
                    input_ids,
                    max_new_tokens=8,
                    do_sample=False,
                    temperature=1.0,
                    pad_token_id=tokenizer.eos_token_id,
                )

            # æå–ç”Ÿæˆçš„ token
            generated_ids = output_ids[0, input_ids.size(1):]
            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

            # æ£€æŸ¥æ˜¯å¦åŒ…å« passkey
            is_correct = passkey in generated_text
            depth_key = str(depth)
            results_by_depth[depth_key]["total"] += 1
            if is_correct:
                results_by_depth[depth_key]["correct"] += 1

            all_results.append({
                "trial": trial,
                "depth": depth,
                "passkey": passkey,
                "generated": generated_text[:50],
                "correct": is_correct,
                "context_tokens": actual_len,
            })

    # æ±‡æ€»
    total_correct = sum(r["correct"] for r in results_by_depth.values())
    total_tests = sum(r["total"] for r in results_by_depth.values())
    overall_accuracy = total_correct / total_tests if total_tests > 0 else 0.0

    # æŒ‰ depth è®¡ç®—å‡†ç¡®ç‡
    depth_accuracy = {}
    for d, r in results_by_depth.items():
        acc = r["correct"] / r["total"] if r["total"] > 0 else 0.0
        depth_accuracy[d] = round(acc, 4)
        print(f"  æ·±åº¦ {d}: {r['correct']}/{r['total']} ({acc:.1%})")

    print(f"  æ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_tests} ({overall_accuracy:.1%})")

    return {
        "accuracy": round(overall_accuracy, 4),
        "depth_accuracy": depth_accuracy,
        "total_correct": total_correct,
        "total_tests": total_tests,
        "context_length": context_length,
        "num_keys": num_keys,
        "details": all_results[:10],  # åªä¿å­˜å‰ 10 æ¡è¯¦æƒ…
    }


def evaluate_generation_ppl(model: Any, tokenizer: Any, config: dict) -> dict:
    """
    Generation PPL è¯„æµ‹ â€” KV Cache å‹åŠ›æµ‹è¯•ã€‚

    ç”¨é•¿ prompt è°ƒç”¨ model.generate() ç”Ÿæˆæ–‡æœ¬,
    ç„¶åç”¨ teacher-forcing è®¡ç®—ç”Ÿæˆéƒ¨åˆ†çš„ PPLã€‚

    KV Cache ä» prompt_length æŒç»­å¢é•¿åˆ° prompt_length + gen_length,
    é‡åŒ–è¯¯å·®éš KV Cache å¢é•¿è€Œç´¯ç§¯ã€‚
    """
    import torch
    import math
    from datasets import load_dataset

    num_prompts = config.get("num_prompts", 5)
    prompt_length = config.get("prompt_length", 1500)
    gen_length = config.get("gen_length", 512)

    # åŠ è½½ PG19 é•¿æ–‡æœ¬
    try:
        dataset = load_dataset("emozilla/pg19-test", split="test")
    except Exception as e:
        print(f"  âŒ åŠ è½½ PG19 å¤±è´¥: {e}")
        return {"gen_ppl": None, "error": str(e)}

    gen_ppls = []
    gen_texts_info = []

    for i in range(min(num_prompts, len(dataset))):
        text = dataset[i]["text"]

        # Tokenize å®Œæ•´æ–‡æœ¬
        full_ids = tokenizer.encode(text, add_special_tokens=True)
        if len(full_ids) < prompt_length + gen_length:
            print(f"  âš ï¸  æ ·æœ¬ {i} å¤ªçŸ­ ({len(full_ids)} tokens), è·³è¿‡")
            continue

        # æˆªå– prompt
        prompt_ids = torch.tensor([full_ids[:prompt_length]], device=model.device)
        # å‚è€ƒç»­å†™ (ç”¨äºè®¡ç®— PPL)
        reference_ids = full_ids[prompt_length:prompt_length + gen_length]

        # Step 1: Generate â€” KV Cache æŒç»­å¢é•¿
        with torch.no_grad():
            output_ids = model.generate(
                prompt_ids,
                max_new_tokens=gen_length,
                do_sample=False,
                temperature=1.0,
                pad_token_id=tokenizer.eos_token_id,
            )

        generated_ids = output_ids[0, prompt_length:]
        generated_text = tokenizer.decode(generated_ids[:50], skip_special_tokens=True)

        # Step 2: è®¡ç®—ç”Ÿæˆæ–‡æœ¬çš„ PPL (teacher-forcing)
        # ç”¨å®Œæ•´åºåˆ— (prompt + reference) åšä¸€æ¬¡ forward, åªè®¡ç®— reference éƒ¨åˆ†çš„ NLL
        full_input = torch.tensor(
            [full_ids[:prompt_length + gen_length]], device=model.device
        )
        labels = full_input.clone()
        labels[:, :prompt_length] = -100  # åªè®¡ç®— reference éƒ¨åˆ†çš„ loss

        with torch.no_grad():
            outputs = model(full_input, labels=labels)
            nll = outputs.loss.item()

        ppl = math.exp(nll)
        gen_ppls.append(ppl)

        gen_texts_info.append({
            "sample": i,
            "prompt_tokens": prompt_length,
            "gen_tokens": len(generated_ids),
            "gen_ppl": round(ppl, 4),
            "generated_preview": generated_text[:100],
        })
        print(f"  æ ·æœ¬ {i}: gen_ppl={ppl:.4f}, gen_tokens={len(generated_ids)}")

    if not gen_ppls:
        return {"gen_ppl": None, "error": "no valid samples"}

    avg_ppl = sum(gen_ppls) / len(gen_ppls)
    print(f"  å¹³å‡ Generation PPL: {avg_ppl:.4f}")

    return {
        "gen_ppl": round(avg_ppl, 4),
        "num_samples": len(gen_ppls),
        "prompt_length": prompt_length,
        "gen_length": gen_length,
        "per_sample": gen_texts_info,
    }

def evaluate_lm_eval(model: Any, tokenizer: Any, tasks: list[str], config: dict) -> dict:
    """
    ä½¿ç”¨ lm-evaluation-harness è¯„æµ‹æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: tokenizer
        tasks: ä»»åŠ¡åç§°åˆ—è¡¨ï¼Œå¦‚ ["mmlu", "hellaswag", "gsm8k"]
        config: é…ç½®å­—å…¸

    è¿”å›:
        dict: æ¯ä¸ªä»»åŠ¡çš„è¯„æµ‹ç»“æœ
    """
    fewshot_map = config.get("common_hyperparams", {}).get("eval_default_fewshot", {})

    try:
        import lm_eval
        from lm_eval.models.huggingface import HFLM

        print(f"  ä½¿ç”¨ lm-eval-harness v{lm_eval.__version__}")

        # åŒ…è£…æ¨¡å‹ä¸º lm-eval æ ¼å¼
        lm = HFLM(pretrained=model, tokenizer=tokenizer)

        # æ„å»ºä»»åŠ¡å‚æ•°
        results = lm_eval.simple_evaluate(
            model=lm,
            tasks=tasks,
            num_fewshot=None,  # ä½¿ç”¨å„ä»»åŠ¡é»˜è®¤å€¼
            batch_size="auto",
        )

        # æå–ç»“æœ
        task_results = {}
        for task_name, task_result in results.get("results", {}).items():
            task_results[task_name] = {
                k: round(v, 4) if isinstance(v, float) else v
                for k, v in task_result.items()
            }

        # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
        accuracies = []
        for task_name, res in task_results.items():
            # lm-eval çš„ç»“æœé”®åå¯èƒ½æ˜¯ acc, acc_norm, exact_match ç­‰
            for key in ("acc,none", "acc_norm,none", "exact_match,none"):
                if key in res:
                    accuracies.append(res[key])
                    break

        if accuracies:
            avg_acc = sum(accuracies) / len(accuracies)
            task_results["_avg_accuracy"] = round(avg_acc, 4)
            print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")

        return task_results

    except ImportError as e:
        print(f"âš ï¸  lm-eval-harness æœªå®‰è£…: {e}")
        print("    è¯·å®‰è£…: pip install lm-eval")
        return {"error": str(e)}
    except Exception as e:
        print(f"âŒ lm-eval è¯„æµ‹å‡ºé”™: {e}")
        return {"error": str(e)}
