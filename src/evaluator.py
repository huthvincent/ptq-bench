# -*- coding: utf-8 -*-
"""
è¯„æµ‹å¼•æ“

è´Ÿè´£å¯¹é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼ŒåŒ…æ‹¬ï¼š
- PPL (Perplexity) è¯„æµ‹ï¼šåœ¨ WikiText-2 / C4 ä¸Šè®¡ç®—å›°æƒ‘åº¦
- lm-eval-harness ä»»åŠ¡è¯„æµ‹ï¼šMMLUã€GSM8Kã€HellaSwag ç­‰
- ç³»ç»ŸæŒ‡æ ‡è¯„æµ‹ï¼ˆPhase 2ï¼‰ï¼šTTFTã€ååé‡ã€VRAM å³°å€¼

ä¸»è¦å…¥å£å‡½æ•°ï¼š
- evaluate(): æ ¹æ®é…ç½®è¿è¡Œå…¨éƒ¨è¯„æµ‹
- evaluate_ppl(): åªè·‘ PPL
- evaluate_lm_eval(): åªè·‘ lm-eval ä»»åŠ¡
"""

import time
import json
from typing import Any
from pathlib import Path


def evaluate(model: Any, tokenizer: Any, config: dict) -> dict:
    """
    æ ¹æ®é…ç½®è¿è¡Œå…¨éƒ¨è¯„æµ‹ã€‚

    è¿™æ˜¯è¯„æµ‹çš„ä¸»å…¥å£å‡½æ•°ï¼Œä¼šæ ¹æ®é…ç½®ä¸­çš„ eval è®¾ç½®
    ä¾æ¬¡è¿è¡Œ PPL è¯„æµ‹å’Œ lm-eval ä»»åŠ¡è¯„æµ‹ã€‚

    å‚æ•°:
        model: é‡åŒ–åï¼ˆæˆ– FP16 baselineï¼‰çš„æ¨¡å‹
        tokenizer: å¯¹åº”çš„ tokenizer
        config: åˆå¹¶åçš„å®Œæ•´é…ç½®å­—å…¸

    è¿”å›:
        dict: è¯„æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å« pplã€lm_eval_resultsã€system_metrics ç­‰
    """
    results = {
        "ppl": {},
        "lm_eval": {},
        "system_metrics": {},
        "eval_time_seconds": 0,
    }

    start_time = time.time()

    # --- PPL è¯„æµ‹ ---
    eval_config = config.get("eval", config.get("default_eval", {}))
    # å…¼å®¹ä¸¤ç§åµŒå¥—ç»“æ„:
    # 1. default_eval é£æ ¼: eval.core_quality.ppl_datasets
    # 2. track é£æ ¼: eval.ppl_datasets (å¹³é“º)
    core_quality = eval_config.get("core_quality", {})
    ppl_datasets = core_quality.get("ppl_datasets", eval_config.get("ppl_datasets", ["wikitext2"]))

    for dataset_name in ppl_datasets:
        print(f"\nğŸ“Š è¯„æµ‹ PPL: {dataset_name}")
        ppl = evaluate_ppl(model, tokenizer, dataset_name, config)
        results["ppl"][dataset_name] = ppl

    # --- lm-eval ä»»åŠ¡è¯„æµ‹ ---
    lm_eval_tasks = core_quality.get("lm_eval_tasks", eval_config.get("lm_eval_tasks", []))
    if lm_eval_tasks:
        print(f"\nğŸ“Š è¯„æµ‹ lm-eval ä»»åŠ¡: {', '.join(lm_eval_tasks)}")
        lm_eval_results = evaluate_lm_eval(model, tokenizer, lm_eval_tasks, config)
        results["lm_eval"] = lm_eval_results

    # --- ç³»ç»ŸæŒ‡æ ‡ï¼ˆPhase 2ï¼‰---
    system_config = eval_config.get("system_metrics", {})
    if isinstance(system_config, dict) and system_config.get("enabled", False):
        print("\nğŸ“Š ç³»ç»ŸæŒ‡æ ‡è¯„æµ‹ï¼ˆPhase 2 åŠŸèƒ½ï¼‰")
        results["system_metrics"] = {"status": "phase2_not_implemented"}

    # --- VRAM å³°å€¼ï¼ˆå¦‚æœ torch å¯ç”¨ï¼‰---
    try:
        import torch
        if torch.cuda.is_available():
            vram_peak_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)
            results["system_metrics"]["vram_peak_mb"] = round(vram_peak_mb, 1)
            print(f"ğŸ“Š VRAM å³°å€¼: {vram_peak_mb:.1f} MB")
    except ImportError:
        pass

    results["eval_time_seconds"] = round(time.time() - start_time, 1)
    return results


def evaluate_ppl(model: Any, tokenizer: Any, dataset_name: str, config: dict) -> dict:
    """
    åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè®¡ç®—æ¨¡å‹çš„ PPL (Perplexity)ã€‚

    ä½¿ç”¨æ ‡å‡†çš„ sliding window æ–¹æ³•è®¡ç®— PPLï¼š
    å°†æµ‹è¯•æ–‡æœ¬ tokenize åï¼ŒæŒ‰ max_seq_len æ»‘åŠ¨çª—å£è®¡ç®— NLLï¼Œ
    æœ€ç»ˆå– exp(avg_nll) ä½œä¸º PPLã€‚

    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: tokenizer
        dataset_name: æ•°æ®é›†åç§°ï¼ˆ"wikitext2" æˆ– "c4"ï¼‰
        config: é…ç½®å­—å…¸

    è¿”å›:
        dict: {"ppl": float, "nll": float, "num_tokens": int}
    """
    max_seq_len = config.get("common_hyperparams", {}).get("max_seq_len", 2048)

    try:
        import torch
        from datasets import load_dataset

        # åŠ è½½æ•°æ®é›†
        if dataset_name == "wikitext2":
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
            text = "\n\n".join(dataset["text"])
        elif dataset_name == "c4":
            dataset = load_dataset("allenai/c4", "en", split="validation", streaming=True)
            # C4 å¾ˆå¤§ï¼Œåªå–å‰ 256 æ¡
            texts = []
            for i, item in enumerate(dataset):
                if i >= 256:
                    break
                texts.append(item["text"])
            text = "\n\n".join(texts)
        else:
            print(f"âš ï¸  æœªçŸ¥æ•°æ®é›† {dataset_name}ï¼Œè·³è¿‡ PPL è¯„æµ‹")
            return {"ppl": None, "error": f"unknown dataset: {dataset_name}"}

        # Tokenize
        encodings = tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids.to(model.device)
        seq_len = input_ids.size(1)

        print(f"  æ•°æ®é›† token æ•°: {seq_len}")

        # Sliding window PPL è®¡ç®—
        nlls = []
        stride = max_seq_len // 2  # ä½¿ç”¨ 50% é‡å 
        for begin in range(0, seq_len - max_seq_len, stride):
            end = begin + max_seq_len
            input_chunk = input_ids[:, begin:end]
            target_chunk = input_chunk.clone()

            # åªè®¡ç®—éé‡å éƒ¨åˆ†çš„ lossï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªçª—å£ï¼‰
            if begin > 0:
                target_chunk[:, :stride] = -100

            with torch.no_grad():
                outputs = model(input_chunk, labels=target_chunk)
                nll = outputs.loss.item()
                nlls.append(nll)

        import math
        avg_nll = sum(nlls) / len(nlls) if nlls else float("inf")
        ppl = math.exp(avg_nll)

        print(f"  PPL: {ppl:.2f}")
        return {"ppl": round(ppl, 4), "nll": round(avg_nll, 6), "num_windows": len(nlls)}

    except ImportError as e:
        print(f"âš ï¸  PPL è¯„æµ‹ä¾èµ–ç¼ºå¤±: {e}")
        return {"ppl": None, "error": str(e)}
    except Exception as e:
        print(f"âŒ PPL è¯„æµ‹å‡ºé”™: {e}")
        return {"ppl": None, "error": str(e)}


def evaluate_lm_eval(model: Any, tokenizer: Any, tasks: list[str], config: dict) -> dict:
    """
    ä½¿ç”¨ lm-evaluation-harness è¯„æµ‹æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: tokenizer
        tasks: ä»»åŠ¡åç§°åˆ—è¡¨ï¼Œå¦‚ ["mmlu", "hellaswag", "gsm8k"]
        config: é…ç½®å­—å…¸

    è¿”å›:
        dict: æ¯ä¸ªä»»åŠ¡çš„è¯„æµ‹ç»“æœ
    """
    fewshot_map = config.get("common_hyperparams", {}).get("eval_default_fewshot", {})

    try:
        import lm_eval
        from lm_eval.models.huggingface import HFLM

        print(f"  ä½¿ç”¨ lm-eval-harness v{lm_eval.__version__}")

        # åŒ…è£…æ¨¡å‹ä¸º lm-eval æ ¼å¼
        lm = HFLM(pretrained=model, tokenizer=tokenizer)

        # æ„å»ºä»»åŠ¡å‚æ•°
        results = lm_eval.simple_evaluate(
            model=lm,
            tasks=tasks,
            num_fewshot=None,  # ä½¿ç”¨å„ä»»åŠ¡é»˜è®¤å€¼
            batch_size="auto",
        )

        # æå–ç»“æœ
        task_results = {}
        for task_name, task_result in results.get("results", {}).items():
            task_results[task_name] = {
                k: round(v, 4) if isinstance(v, float) else v
                for k, v in task_result.items()
            }

        # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
        accuracies = []
        for task_name, res in task_results.items():
            # lm-eval çš„ç»“æœé”®åå¯èƒ½æ˜¯ acc, acc_norm, exact_match ç­‰
            for key in ("acc,none", "acc_norm,none", "exact_match,none"):
                if key in res:
                    accuracies.append(res[key])
                    break

        if accuracies:
            avg_acc = sum(accuracies) / len(accuracies)
            task_results["_avg_accuracy"] = round(avg_acc, 4)
            print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")

        return task_results

    except ImportError as e:
        print(f"âš ï¸  lm-eval-harness æœªå®‰è£…: {e}")
        print("    è¯·å®‰è£…: pip install lm-eval")
        return {"error": str(e)}
    except Exception as e:
        print(f"âŒ lm-eval è¯„æµ‹å‡ºé”™: {e}")
        return {"error": str(e)}
