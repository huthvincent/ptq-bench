#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_all.py â€” æ‰¹é‡è¿è¡Œå¤šä¸ªå®éªŒ

æ”¯æŒä¸¤ç§æ¨¡å¼:
1. ç¬›å¡å°”ç§¯: æŒ‡å®š models Ã— methods Ã— tracks çš„æ‰€æœ‰ç»„åˆ
2. å®éªŒé…ç½®: ä» experiment YAML è¯»å–å…·ä½“ç»„åˆ

ç”¨æ³•ç¤ºä¾‹:
    # è·‘æ‰€æœ‰ Track A æ–¹æ³• Ã— æ‰€æœ‰æ¨¡å‹
    python scripts/run_all.py --include_tracks A

    # ä½¿ç”¨å®éªŒé…ç½®
    python scripts/run_all.py --experiment configs/experiments/quick_test.yaml

    # è·³è¿‡å·²æœ‰ç»“æœ
    python scripts/run_all.py --include_tracks A --resume
"""

import sys
import argparse
import itertools
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src import registry
from src.config import load_global_config, load_experiment_config, get_project_root
from src.runner import run_experiment


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(
        description="PTQ Benchmark: æ‰¹é‡è¿è¡Œå¤šä¸ªå®éªŒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # === ç¬›å¡å°”ç§¯æ¨¡å¼ ===
    parser.add_argument("--include_models", nargs="+", default=None,
                        help="è¦åŒ…å«çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰")
    parser.add_argument("--exclude_models", nargs="+", default=None,
                        help="è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--include_methods", nargs="+", default=None,
                        help="è¦åŒ…å«çš„æ–¹æ³•åˆ—è¡¨ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰")
    parser.add_argument("--exclude_methods", nargs="+", default=None,
                        help="è¦æ’é™¤çš„æ–¹æ³•åˆ—è¡¨")
    parser.add_argument("--include_tracks", nargs="+", default=None,
                        choices=["A", "B", "C"],
                        help="è¦åŒ…å«çš„èµ›é“åˆ—è¡¨ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰")

    # === å®éªŒé…ç½®æ¨¡å¼ ===
    parser.add_argument("--experiment", type=str, default=None,
                        help="å®éªŒé…ç½® YAML è·¯å¾„")

    # === è¿è¡Œæ§åˆ¶ ===
    parser.add_argument("--max_jobs", type=int, default=1,
                        help="æœ€å¤§å¹¶å‘æ•°ï¼ˆå½“å‰ä»…æ”¯æŒé¡ºåºæ‰§è¡Œ=1ï¼‰")
    parser.add_argument("--resume", action="store_true",
                        help="è·³è¿‡å·²æœ‰ç»“æœæ–‡ä»¶çš„å®éªŒ")
    parser.add_argument("--output_dir", type=str, default=None,
                        help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--dry_run", action="store_true",
                        help="åªæ‰“å°è®¡åˆ’è¿è¡Œçš„å®éªŒåˆ—è¡¨ï¼Œä¸å®é™…è¿è¡Œ")

    return parser.parse_args()


def discover_available_configs(project_root: Path) -> dict:
    """
    æ‰«æ configs/ ç›®å½•ï¼Œå‘ç°æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ã€æ–¹æ³•é…ç½®ã€‚

    å‚æ•°:
        project_root: é¡¹ç›®æ ¹ç›®å½•

    è¿”å›:
        dict: {"models": [...], "methods": [...]}
    """
    models_dir = project_root / "configs" / "models"
    methods_dir = project_root / "configs" / "methods"

    models = sorted([
        f.stem for f in models_dir.glob("*.yaml") if f.stem != "README"
    ])
    methods = sorted([
        f.stem for f in methods_dir.glob("*.yaml") if f.stem != "README"
    ])

    return {"models": models, "methods": methods}


def check_existing_results(output_dir: Path, model: str, method: str, track: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥å®éªŒçš„ç»“æœæ–‡ä»¶ã€‚

    å‚æ•°:
        output_dir: ç»“æœç›®å½•
        model: æ¨¡å‹å
        method: æ–¹æ³•å
        track: èµ›é“å

    è¿”å›:
        bool: æ˜¯å¦å­˜åœ¨ç»“æœ
    """
    pattern = f"*__{model}__{method}__track{track.upper()}.json"
    return bool(list(output_dir.glob(pattern)))


def main():
    """ä¸»å…¥å£å‡½æ•°ã€‚"""
    args = parse_args()
    project_root = get_project_root()

    # è‡ªåŠ¨å‘ç°å¹¶æ³¨å†Œæ‰€æœ‰é‡åŒ–æ–¹æ³•
    registry.auto_discover()

    # ================================================================
    # æ„å»ºå®éªŒç»„åˆåˆ—è¡¨
    # ================================================================
    if args.experiment:
        # ä» experiment YAML è¯»å–
        exp_config = load_experiment_config(args.experiment, project_root)
        models = exp_config.get("models", [])
        methods = exp_config.get("methods", [])
        tracks = exp_config.get("tracks", ["A"])
    else:
        # ä» configs/ ç›®å½•å‘ç°
        available = discover_available_configs(project_root)
        models = args.include_models or available["models"]
        methods = args.include_methods or available["methods"]
        tracks = args.include_tracks or ["A", "B", "C"]

    # åº”ç”¨æ’é™¤è¿‡æ»¤
    if args.exclude_models:
        models = [m for m in models if m not in args.exclude_models]
    if args.exclude_methods:
        methods = [m for m in methods if m not in args.exclude_methods]

    # æ„å»ºç¬›å¡å°”ç§¯ï¼Œå¹¶è¿‡æ»¤ä¸å…¼å®¹çš„ç»„åˆï¼ˆæ–¹æ³•ä¸æ”¯æŒçš„ Track è·³è¿‡ï¼‰
    experiments = []
    for model, method, track in itertools.product(models, methods, tracks):
        # æ£€æŸ¥æ–¹æ³•æ˜¯å¦æ”¯æŒè¯¥èµ›é“
        try:
            method_cls = registry.get(method)
            supported = getattr(method_cls, "supported_tracks", [])
            if track.upper() not in [t.upper() for t in supported]:
                continue
        except KeyError:
            continue
        experiments.append((model, method, track))

    # ================================================================
    # è¾“å‡ºè®¡åˆ’
    # ================================================================
    output_dir = Path(args.output_dir) if args.output_dir else project_root / "results"

    print("=" * 60)
    print(f"ğŸš€ PTQ Benchmark: run_all")
    print(f"   æ¨¡å‹: {models}")
    print(f"   æ–¹æ³•: {methods}")
    print(f"   èµ›é“: {tracks}")
    print(f"   æ€»å®éªŒæ•°: {len(experiments)}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)

    if args.dry_run:
        print("\nğŸ” [DRY RUN] è®¡åˆ’è¿è¡Œçš„å®éªŒ:")
        for i, (model, method, track) in enumerate(experiments, 1):
            skip = ""
            if args.resume and check_existing_results(output_dir, model, method, track):
                skip = " [è·³è¿‡: å·²æœ‰ç»“æœ]"
            print(f"  {i}. {model} Ã— {method} Ã— Track {track}{skip}")
        print(f"\næ€»è®¡: {len(experiments)} ä¸ªå®éªŒ")
        return

    # ================================================================
    # é¡ºåºæ‰§è¡Œå®éªŒ
    # ================================================================
    completed = 0
    skipped = 0
    failed = 0

    for i, (model, method, track) in enumerate(experiments, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ å®éªŒ {i}/{len(experiments)}: {model} Ã— {method} Ã— Track {track}")
        print(f"{'='*60}")

        # resume æ¨¡å¼: è·³è¿‡å·²æœ‰ç»“æœ
        if args.resume and check_existing_results(output_dir, model, method, track):
            print(f"â­ï¸  è·³è¿‡: å·²æœ‰ç»“æœæ–‡ä»¶")
            skipped += 1
            continue

        try:
            cli_args_str = f"python scripts/run_all.py {' '.join(sys.argv[1:])}"
            result = run_experiment(
                model_name=model,
                method_name=method,
                track=track,
                output_dir=output_dir,
                cli_args_str=cli_args_str,
                script_name="scripts/run_all.py",
            )
            if result:
                completed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            failed += 1

    # ================================================================
    # æ±‡æ€»
    # ================================================================
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ‰¹é‡è¿è¡Œæ±‡æ€»:")
    print(f"   âœ… å®Œæˆ: {completed}")
    print(f"   â­ï¸  è·³è¿‡: {skipped}")
    print(f"   âŒ å¤±è´¥: {failed}")
    print(f"   æ€»è®¡: {len(experiments)}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
