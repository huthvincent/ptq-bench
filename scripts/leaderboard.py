#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
leaderboard.py â€” ä» results/ ç›®å½•æ±‡æ€»ç”Ÿæˆæ’è¡Œæ¦œ

æ‰«ææ‰€æœ‰ .json ç»“æœæ–‡ä»¶ï¼ŒæŒ‰ Track åˆ†ç»„ï¼Œ
ç”Ÿæˆ results/leaderboard.md æ’è¡Œæ¦œï¼Œ
å¹¶æ›´æ–° summary.md çš„ "å½“å‰æœ€å¥½ç»“æœ" åŒºåŸŸã€‚

ç”¨æ³•:
    python scripts/leaderboard.py
    python scripts/leaderboard.py --results_dir results/ --top_k 3
"""

import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(description="PTQ Benchmark: ç”Ÿæˆæ’è¡Œæ¦œ")
    parser.add_argument("--results_dir", type=str, default="results",
                        help="ç»“æœæ–‡ä»¶ç›®å½• (é»˜è®¤: results/)")
    parser.add_argument("--output", type=str, default="results/leaderboard.md",
                        help="æ’è¡Œæ¦œè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--top_k", type=int, default=5,
                        help="æ¯ä¸ª Track æ¯ä¸ªæ¨¡å‹å±•ç¤º top-k ç»“æœ")
    parser.add_argument("--update_summary", action="store_true", default=True,
                        help="åŒæ—¶æ›´æ–° summary.md çš„æœ€å¥½ç»“æœåŒºåŸŸ")
    return parser.parse_args()


def load_all_results(results_dir: Path) -> list[dict]:
    """
    åŠ è½½ results/ ç›®å½•ä¸‹æ‰€æœ‰ .json ç»“æœæ–‡ä»¶ã€‚

    å‚æ•°:
        results_dir: ç»“æœæ–‡ä»¶ç›®å½•

    è¿”å›:
        list[dict]: æ‰€æœ‰ç»“æœæ•°æ®åˆ—è¡¨
    """
    results = []
    for json_file in sorted(results_dir.glob("*.json")):
        if json_file.name == "leaderboard.json":
            continue  # è·³è¿‡æ’è¡Œæ¦œè‡ªèº«
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            data["_source_file"] = json_file.name
            results.append(data)
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸  è·³è¿‡æŸåçš„æ–‡ä»¶ {json_file.name}: {e}")
    return results


def extract_key_metrics(result: dict) -> dict:
    """
    ä»ç»“æœæ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡ã€‚

    å‚æ•°:
        result: å•ä¸ªå®éªŒç»“æœå­—å…¸

    è¿”å›:
        dict: å…³é”®æŒ‡æ ‡æ‘˜è¦
    """
    meta = result.get("meta", {})
    results_data = result.get("results", {})

    metrics = {
        "model": meta.get("model", "?"),
        "method": meta.get("method", "?"),
        "track": meta.get("track", "?"),
        "timestamp": meta.get("timestamp", ""),
        "source_file": result.get("_source_file", ""),
    }

    # PPL
    ppl_data = results_data.get("ppl", {})
    for dataset, vals in ppl_data.items():
        if isinstance(vals, dict) and "ppl" in vals:
            metrics[f"ppl_{dataset}"] = vals["ppl"]

    # lm-eval å¹³å‡å‡†ç¡®ç‡
    lm_eval = results_data.get("lm_eval", {})
    if "_avg_accuracy" in lm_eval:
        metrics["avg_accuracy"] = lm_eval["_avg_accuracy"]

    # å„ä»»åŠ¡åˆ†æ•°
    for task, task_res in lm_eval.items():
        if task.startswith("_") or not isinstance(task_res, dict):
            continue
        for key in ("acc,none", "acc_norm,none", "exact_match,none"):
            if key in task_res:
                metrics[f"lm_{task}"] = task_res[key]
                break

    # ç³»ç»ŸæŒ‡æ ‡
    sys_metrics = results_data.get("system_metrics", {})
    if "vram_peak_mb" in sys_metrics:
        metrics["vram_peak_mb"] = sys_metrics["vram_peak_mb"]

    # è­¦å‘Šæ ‡è®°
    warnings = result.get("warnings", [])
    metrics["has_warnings"] = len(warnings) > 0

    return metrics


def generate_leaderboard(all_results: list[dict], top_k: int = 5) -> str:
    """
    ç”Ÿæˆæ’è¡Œæ¦œ Markdown å†…å®¹ã€‚

    å‚æ•°:
        all_results: æ‰€æœ‰ç»“æœæ•°æ®
        top_k: æ¯ä¸ª Track æ¯ä¸ªæ¨¡å‹å±•ç¤ºå‰ k å

    è¿”å›:
        str: Markdown æ ¼å¼çš„æ’è¡Œæ¦œ
    """
    # æå–æŒ‡æ ‡
    all_metrics = [extract_key_metrics(r) for r in all_results]

    # æŒ‰ Track åˆ†ç»„
    by_track = defaultdict(list)
    for m in all_metrics:
        by_track[m["track"]].append(m)

    lines = []
    lines.append("# ğŸ“Š PTQ Benchmark æ’è¡Œæ¦œ")
    lines.append("")
    lines.append(f"*è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    lines.append("")
    lines.append(f"æ€»å®éªŒæ•°: {len(all_metrics)}")
    lines.append("")

    for track in sorted(by_track.keys()):
        track_data = by_track[track]
        lines.append(f"## Track {track}")
        lines.append("")

        # æŒ‰æ¨¡å‹åˆ†ç»„
        by_model = defaultdict(list)
        for m in track_data:
            by_model[m["model"]].append(m)

        for model_name in sorted(by_model.keys()):
            model_data = by_model[model_name]

            # æŒ‰ avg_accuracy æ’åºï¼ˆé™åºï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŒ‰ PPL æ’åºï¼ˆå‡åºï¼‰
            def sort_key(m):
                if "avg_accuracy" in m:
                    return -m["avg_accuracy"]  # è´Ÿå·ä½¿é™åº
                if "ppl_wikitext2" in m:
                    return m["ppl_wikitext2"]  # PPL è¶Šä½è¶Šå¥½
                return float("inf")

            model_data.sort(key=sort_key)
            model_data = model_data[:top_k]

            lines.append(f"### {model_name}")
            lines.append("")

            # ç”Ÿæˆè¡¨æ ¼
            header = "| æ’å | æ–¹æ³• | PPL (WikiText-2) | Avg Accuracy | VRAM (MB) | âš ï¸ | ç»“æœæ–‡ä»¶ |"
            separator = "|------|------|-----------------|-------------|-----------|---|---------|"
            lines.append(header)
            lines.append(separator)

            for rank, m in enumerate(model_data, 1):
                ppl = m.get("ppl_wikitext2", "-")
                if isinstance(ppl, float):
                    ppl = f"{ppl:.2f}"
                acc = m.get("avg_accuracy", "-")
                if isinstance(acc, float):
                    acc = f"{acc:.4f}"
                vram = m.get("vram_peak_mb", "-")
                if isinstance(vram, float):
                    vram = f"{vram:.0f}"
                warn = "âš ï¸" if m.get("has_warnings") else ""
                source = m.get("source_file", "")
                lines.append(f"| {rank} | {m['method']} | {ppl} | {acc} | {vram} | {warn} | {source} |")

            lines.append("")

    if not by_track:
        lines.append("*æš‚æ— å®éªŒç»“æœã€‚è¯·å…ˆè¿è¡Œ `bash scripts/run_one.sh` ç”Ÿæˆç»“æœã€‚*")
        lines.append("")

    return "\n".join(lines)


def generate_summary_snippet(all_results: list[dict]) -> str:
    """
    ç”Ÿæˆç”¨äº summary.md çš„æœ€å¥½ç»“æœæ‘˜è¦ã€‚

    å‚æ•°:
        all_results: æ‰€æœ‰ç»“æœæ•°æ®

    è¿”å›:
        str: æ‘˜è¦æ–‡æœ¬
    """
    all_metrics = [extract_key_metrics(r) for r in all_results]

    lines = []
    lines.append("### å½“å‰æœ€å¥½ç»“æœ")
    lines.append("")
    lines.append(f"*æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    lines.append("")

    by_track = defaultdict(list)
    for m in all_metrics:
        by_track[m["track"]].append(m)

    for track in sorted(by_track.keys()):
        track_data = by_track[track]

        def sort_key(m):
            if "avg_accuracy" in m:
                return -m["avg_accuracy"]
            if "ppl_wikitext2" in m:
                return m["ppl_wikitext2"]
            return float("inf")

        track_data.sort(key=sort_key)
        if track_data:
            best = track_data[0]
            ppl = best.get("ppl_wikitext2", "N/A")
            acc = best.get("avg_accuracy", "N/A")
            lines.append(f"- **Track {track}** æœ€ä½³: {best['method']} on {best['model']} (PPL={ppl}, Acc={acc})")

    lines.append("")
    lines.append("è¯¦ç»†æ’è¡Œæ¦œè§ [results/leaderboard.md](results/leaderboard.md)")

    return "\n".join(lines)


def main():
    """ä¸»å…¥å£å‡½æ•°ã€‚"""
    args = parse_args()

    results_dir = Path(args.results_dir)
    if not results_dir.is_absolute():
        results_dir = PROJECT_ROOT / results_dir

    print(f"ğŸ“Š æ‰«æç»“æœç›®å½•: {results_dir}")
    all_results = load_all_results(results_dir)
    print(f"   æ‰¾åˆ° {len(all_results)} ä¸ªç»“æœæ–‡ä»¶")

    # ç”Ÿæˆæ’è¡Œæ¦œ
    leaderboard_md = generate_leaderboard(all_results, top_k=args.top_k)

    output_path = Path(args.output)
    if not output_path.is_absolute():
        output_path = PROJECT_ROOT / output_path

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(leaderboard_md)
    print(f"âœ… æ’è¡Œæ¦œå·²ç”Ÿæˆ: {output_path}")

    # æ›´æ–° summary.md
    if args.update_summary and all_results:
        snippet = generate_summary_snippet(all_results)
        print(f"\nğŸ“‹ summary.md æœ€å¥½ç»“æœæ‘˜è¦:")
        print(snippet)
        print("\nğŸ’¡ è¯·æ‰‹åŠ¨å°†ä»¥ä¸Šå†…å®¹æ›´æ–°åˆ° summary.md çš„ 'å½“å‰æœ€å¥½ç»“æœ' åŒºåŸŸ")


if __name__ == "__main__":
    main()
