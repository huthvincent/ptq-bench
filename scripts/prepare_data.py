#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
prepare_data.py â€” å‡†å¤‡æ ¡å‡†æ•°æ®

ä¸‹è½½å¹¶é¢„å¤„ç†æ ¡å‡†æ•°æ®é›†ï¼ˆWikiText-2 / C4ï¼‰ï¼Œ
å°† tokenized + packed çš„ token blocks ä¿å­˜åˆ° data/processed/ï¼Œ
ç¡®ä¿ä¸åŒé‡åŒ–æ–¹æ³•ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ ¡å‡†æ•°æ®ã€‚

ç”¨æ³•:
    python scripts/prepare_data.py --dataset wikitext2 --model llama3.1-8b
    python scripts/prepare_data.py --dataset c4 --num_samples 512 --seq_len 2048

æ³¨æ„: Phase 1 ä¸­å¤§å¤šæ•°æ–¹æ³•å†…éƒ¨è‡ªå·±å¤„ç†æ ¡å‡†æ•°æ®ï¼Œ
      æ­¤è„šæœ¬åœ¨éœ€è¦ä¸¥æ ¼æ§åˆ¶æ ¡å‡†æ•°æ®ä¸€è‡´æ€§æ—¶ä½¿ç”¨ã€‚
"""

import sys
import argparse
import hashlib
import json
from pathlib import Path
from datetime import datetime

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(description="PTQ Benchmark: å‡†å¤‡æ ¡å‡†æ•°æ®")
    parser.add_argument("--dataset", type=str, default="wikitext2",
                        choices=["wikitext2", "c4"],
                        help="æ ¡å‡†æ•°æ®é›†")
    parser.add_argument("--model", type=str, default="llama3.1-8b",
                        help="ç”¨å“ªä¸ªæ¨¡å‹çš„ tokenizer")
    parser.add_argument("--num_samples", type=int, default=128,
                        help="æ ¡å‡†æ ·æœ¬æ•°")
    parser.add_argument("--seq_len", type=int, default=2048,
                        help="æ¯ä¸ªæ ·æœ¬çš„åºåˆ—é•¿åº¦")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    parser.add_argument("--output_dir", type=str, default="/data2/zhu11/quant_source/data/processed/calibration",
                        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: å…±äº«æ•°æ®ç›®å½•ï¼‰")
    return parser.parse_args()


def main():
    """ä¸»å…¥å£å‡½æ•°ã€‚"""
    args = parse_args()

    output_dir = Path(args.output_dir)
    if not output_dir.is_absolute():
        output_dir = PROJECT_ROOT / output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“¦ å‡†å¤‡æ ¡å‡†æ•°æ®")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   æ¨¡å‹ tokenizer: {args.model}")
    print(f"   æ ·æœ¬æ•°: {args.num_samples}")
    print(f"   åºåˆ—é•¿åº¦: {args.seq_len}")
    print(f"   éšæœºç§å­: {args.seed}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")

    try:
        from src.config import load_model_config
        model_config = load_model_config(args.model, PROJECT_ROOT)
        model_id = model_config["model_id"]

        from transformers import AutoTokenizer
        print(f"\nğŸ“¦ åŠ è½½ tokenizer: {model_id}")
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        from src.runner import _prepare_calibration_data
        config = {
            "calibration": {
                "dataset": args.dataset,
                "num_samples": args.num_samples,
                "seq_len": args.seq_len,
                "seed": args.seed,
            }
        }

        calib_data = _prepare_calibration_data(tokenizer, config)

        # ä¿å­˜
        import torch
        output_name = f"{args.dataset}_{args.num_samples}x{args.seq_len}_seed{args.seed}"
        output_file = output_dir / f"{output_name}.pt"
        torch.save(calib_data, output_file)
        print(f"\nâœ… æ ¡å‡†æ•°æ®å·²ä¿å­˜: {output_file}")

        # ä¿å­˜å…ƒæ•°æ®
        meta = {
            "dataset": args.dataset,
            "model_tokenizer": args.model,
            "model_id": model_id,
            "num_samples": len(calib_data),
            "seq_len": args.seq_len,
            "seed": args.seed,
            "timestamp": datetime.now().isoformat(),
            "file": str(output_file),
        }
        meta_dir = Path("/data2/zhu11/quant_source/data/meta")
        meta_dir.mkdir(parents=True, exist_ok=True)
        meta_file = meta_dir / f"{output_name}.json"
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {meta_file}")

    except ImportError as e:
        print(f"âŒ ä¾èµ–ç¼ºå¤±: {e}")
        print("   è¯·å®‰è£…: pip install transformers torch datasets")
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
